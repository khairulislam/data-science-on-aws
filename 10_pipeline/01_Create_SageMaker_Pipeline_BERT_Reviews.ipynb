{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A SageMaker Workflow\n",
    "\n",
    "The pipeline that we create follows a typical Machine Learning Application pattern of pre-processing, training, evaluation, and model registration:\n",
    "\n",
    "![A typical ML Application pipeline](img/pipeline-full.png)\n",
    "\n",
    "### Create SageMaker Clients and Session\n",
    "\n",
    "First, we create a new SageMaker Session in the current AWS region. We also acquire the role arn for the session.\n",
    "\n",
    "This role arn should be the execution role arn that you set up in the Prerequisites section of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import os\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track the Pipeline as an `Experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "pipeline_name = \"BERT-pipeline-{}\".format(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline experiment name: BERT-pipeline-1710454714\n"
     ]
    }
   ],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "pipeline_experiment = Experiment.create(\n",
    "    experiment_name=pipeline_name,\n",
    "    description=\"Amazon Customer Reviews BERT Pipeline Experiment\",\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "\n",
    "pipeline_experiment_name = pipeline_experiment.experiment_name\n",
    "print(\"Pipeline experiment name: {}\".format(pipeline_experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_experiment_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pipeline_experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the `Trial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial name: trial-1710454714\n"
     ]
    }
   ],
   "source": [
    "from smexperiments.trial import Trial\n",
    "\n",
    "pipeline_trial = Trial.create(\n",
    "    trial_name=\"trial-{}\".format(timestamp), experiment_name=pipeline_experiment_name, sagemaker_boto_client=sm\n",
    ")\n",
    "\n",
    "pipeline_trial_name = pipeline_trial.trial_name\n",
    "print(\"Trial name: {}\".format(pipeline_trial_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_trial_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pipeline_trial_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Parameters to Parametrize Pipeline Execution\n",
    "\n",
    "We define Workflow Parameters by which we can parametrize our Pipeline and vary the values injected and used in Pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* `ParameterString` - representing a `str` Python type\n",
    "* `ParameterInteger` - representing an `int` Python type\n",
    "* `ParameterFloat` - representing a `float` Python type\n",
    "\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution. The default value specified should be an instance of the type of the parameter.\n",
    "\n",
    "The parameters defined in this workflow below include:\n",
    "\n",
    "* `processing_instance_type` - The `ml.*` instance type of the processing job.\n",
    "* `processing_instance_count` - The instance count of the processing job. For illustrative purposes only: 1 is the only value that makes sense here.\n",
    "* `train_instance_type` - The `ml.*` instance type of the training job.\n",
    "* `model_approval_status` - What approval status to register the trained model with for CI/CD purposes. Defaults to \"PendingManualApproval\". (NOTE: not available in service yet)\n",
    "* `input_data` - The URL location of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = ParameterString(\n",
    "    name=\"ExperimentName\",\n",
    "    default_value=pipeline_experiment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Step Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Processing Step for Feature Engineering](img/pipeline-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-211125778552/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "raw_input_data_s3_uri = \"s3://{}/amazon-reviews-pds/tsv/\".format(bucket)\n",
    "print(raw_input_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-10 04:20:47   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\n",
      "2024-03-10 04:20:48   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\n",
      "2024-03-10 04:20:50   12134676 amazon_reviews_us_Gift_Card_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $raw_input_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=raw_input_data_s3_uri,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=\"ml.c5.2xlarge\")\n",
    "\n",
    "max_seq_length = ParameterInteger(\n",
    "    name=\"MaxSeqLength\",\n",
    "    default_value=64,\n",
    ")\n",
    "\n",
    "balance_dataset = ParameterString(\n",
    "    name=\"BalanceDataset\",\n",
    "    default_value=\"True\",\n",
    ")\n",
    "\n",
    "train_split_percentage = ParameterFloat(\n",
    "    name=\"TrainSplitPercentage\",\n",
    "    default_value=0.90,\n",
    ")\n",
    "\n",
    "validation_split_percentage = ParameterFloat(\n",
    "    name=\"ValidationSplitPercentage\",\n",
    "    default_value=0.05,\n",
    ")\n",
    "\n",
    "test_split_percentage = ParameterFloat(\n",
    "    name=\"TestSplitPercentage\",\n",
    "    default_value=0.05,\n",
    ")\n",
    "\n",
    "feature_store_offline_prefix = ParameterString(\n",
    "    name=\"FeatureStoreOfflinePrefix\",\n",
    "    default_value=\"reviews-feature-store-\" + str(timestamp),\n",
    ")\n",
    "\n",
    "feature_group_name = ParameterString(name=\"FeatureGroupName\", default_value=\"reviews-feature-group-\" + str(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gmtime, strftime, sleep\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m## PIP INSTALLS ##\u001b[39;49;00m\n",
      "\u001b[37m# This is 2.3.0 (vs. 2.3.1 everywhere else) because we need to\u001b[39;49;00m\n",
      "\u001b[37m# use anaconda and anaconda only supports 2.3.0 at this time\u001b[39;49;00m\n",
      "\u001b[37m#subprocess.check_call([sys.executable, \"-m\", \"conda\", \"install\", \"-c\", \"anaconda\", \"tensorflow==2.3.0\", \"-y\"])\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtensorflow==2.3.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mconda-forge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\n",
      "\n",
      "\u001b[37m#subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.24.1\"])\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[37m# import sagemaker\u001b[39;49;00m\n",
      "\u001b[37m# from sagemaker.session import Session\u001b[39;49;00m\n",
      "\u001b[37m# from sagemaker.feature_store.feature_group import FeatureGroup\u001b[39;49;00m\n",
      "\u001b[37m# from sagemaker.feature_store.feature_definition import (\u001b[39;49;00m\n",
      "\u001b[37m#     FeatureDefinition,\u001b[39;49;00m\n",
      "\u001b[37m#     FeatureTypeEnum,\u001b[39;49;00m\n",
      "\u001b[37m# )\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# region = os.environ[\"AWS_DEFAULT_REGION\"]\u001b[39;49;00m\n",
      "\u001b[37m# print(\"Region: {}\".format(region))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# #############################\u001b[39;49;00m\n",
      "\u001b[37m# ## We may need to get the Role and Bucket before setting sm, featurestore_runtime, etc.\u001b[39;49;00m\n",
      "\u001b[37m# ## Role and Bucket are malformed if we do this later.\u001b[39;49;00m\n",
      "\u001b[37m# sts = boto3.Session(region_name=region).client(service_name=\"sts\", region_name=region)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# caller_identity = sts.get_caller_identity()\u001b[39;49;00m\n",
      "\u001b[37m# print(\"caller_identity: {}\".format(caller_identity))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# assumed_role_arn = caller_identity[\"Arn\"]\u001b[39;49;00m\n",
      "\u001b[37m# print(\"(assumed_role) caller_identity_arn: {}\".format(assumed_role_arn))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# assumed_role_name = assumed_role_arn.split(\"/\")[-2]\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# iam = boto3.Session(region_name=region).client(service_name=\"iam\", region_name=region)\u001b[39;49;00m\n",
      "\u001b[37m# get_role_response = iam.get_role(RoleName=assumed_role_name)\u001b[39;49;00m\n",
      "\u001b[37m# print(\"get_role_response {}\".format(get_role_response))\u001b[39;49;00m\n",
      "\u001b[37m# role = get_role_response[\"Role\"][\"Arn\"]\u001b[39;49;00m\n",
      "\u001b[37m# print(\"role {}\".format(role))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# bucket = sagemaker.Session().default_bucket()\u001b[39;49;00m\n",
      "\u001b[37m# print(\"The DEFAULT BUCKET is {}\".format(bucket))\u001b[39;49;00m\n",
      "\u001b[37m# #############################\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# sm = boto3.Session(region_name=region).client(service_name=\"sagemaker\", region_name=region)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# featurestore_runtime = boto3.Session(region_name=region).client(\u001b[39;49;00m\n",
      "\u001b[37m#     service_name=\"sagemaker-featurestore-runtime\", region_name=region\u001b[39;49;00m\n",
      "\u001b[37m# )\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# s3 = boto3.Session(region_name=region).client(service_name=\"s3\", region_name=region)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# sagemaker_session = sagemaker.Session(\u001b[39;49;00m\n",
      "\u001b[37m#     boto_session=boto3.Session(region_name=region),\u001b[39;49;00m\n",
      "\u001b[37m#     sagemaker_client=sm,\u001b[39;49;00m\n",
      "\u001b[37m#     sagemaker_featurestore_runtime_client=featurestore_runtime,\u001b[39;49;00m\n",
      "\u001b[37m# )\u001b[39;49;00m\n",
      "\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "REVIEW_BODY_COLUMN = \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "REVIEW_ID_COLUMN = \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\u001b[37m# DATE_COLUMN = 'date'\u001b[39;49;00m\n",
      "\n",
      "LABEL_COLUMN = \u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "\n",
      "label_map = {}\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\n",
      "    label_map[label] = i\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcast_object_to_string\u001b[39;49;00m(data_frame):\n",
      "    \u001b[34mfor\u001b[39;49;00m label \u001b[35min\u001b[39;49;00m data_frame.columns:\n",
      "        \u001b[34mif\u001b[39;49;00m data_frame.dtypes[label] == \u001b[33m\"\u001b[39;49;00m\u001b[33mobject\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "            data_frame[label] = data_frame[label].astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).astype(\u001b[33m\"\u001b[39;49;00m\u001b[33mstring\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m data_frame\n",
      "\n",
      "\n",
      "\u001b[37m# def wait_for_feature_group_creation_complete(feature_group):\u001b[39;49;00m\n",
      "\u001b[37m#     try:\u001b[39;49;00m\n",
      "\u001b[37m#         status = feature_group.describe().get(\"FeatureGroupStatus\")\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"Feature Group status: {}\".format(status))\u001b[39;49;00m\n",
      "\u001b[37m#         while status == \"Creating\":\u001b[39;49;00m\n",
      "\u001b[37m#             print(\"Waiting for Feature Group Creation\")\u001b[39;49;00m\n",
      "\u001b[37m#             time.sleep(5)\u001b[39;49;00m\n",
      "\u001b[37m#             status = feature_group.describe().get(\"FeatureGroupStatus\")\u001b[39;49;00m\n",
      "\u001b[37m#             print(\"Feature Group status: {}\".format(status))\u001b[39;49;00m\n",
      "\u001b[37m#         if status != \"Created\":\u001b[39;49;00m\n",
      "\u001b[37m#             print(\"Feature Group status: {}\".format(status))\u001b[39;49;00m\n",
      "\u001b[37m#             raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\u001b[39;49;00m\n",
      "\u001b[37m#         print(f\"FeatureGroup {feature_group.name} successfully created.\")\u001b[39;49;00m\n",
      "\u001b[37m#     except:\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"No feature group created yet.\")\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[37m# def create_or_load_feature_group(prefix, feature_group_name):\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     # Feature Definitions for our records\u001b[39;49;00m\n",
      "\u001b[37m#     feature_definitions = [\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"input_ids\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"input_mask\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"segment_ids\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"label_id\", feature_type=FeatureTypeEnum.INTEGRAL),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"review_id\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"date\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"label\", feature_type=FeatureTypeEnum.INTEGRAL),\u001b[39;49;00m\n",
      "\u001b[37m#         #        FeatureDefinition(feature_name='review_body', feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#         FeatureDefinition(feature_name=\"split_type\", feature_type=FeatureTypeEnum.STRING),\u001b[39;49;00m\n",
      "\u001b[37m#     ]\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     feature_group = FeatureGroup(\u001b[39;49;00m\n",
      "\u001b[37m#         name=feature_group_name, feature_definitions=feature_definitions, sagemaker_session=sagemaker_session\u001b[39;49;00m\n",
      "\u001b[37m#     )\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     print(\"Feature Group: {}\".format(feature_group))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     try:\u001b[39;49;00m\n",
      "\u001b[37m#         print(\u001b[39;49;00m\n",
      "\u001b[37m#             \"Waiting for existing Feature Group to become available if it is being created by another instance in our cluster...\"\u001b[39;49;00m\n",
      "\u001b[37m#         )\u001b[39;49;00m\n",
      "\u001b[37m#         wait_for_feature_group_creation_complete(feature_group)\u001b[39;49;00m\n",
      "\u001b[37m#     except Exception as e:\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"Before CREATE FG wait exeption: {}\".format(e))\u001b[39;49;00m\n",
      "\u001b[37m#     #        pass\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     try:\u001b[39;49;00m\n",
      "\u001b[37m#         record_identifier_feature_name = \"review_id\"\u001b[39;49;00m\n",
      "\u001b[37m#         event_time_feature_name = \"date\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#         print(\"Creating Feature Group with role {}...\".format(role))\u001b[39;49;00m\n",
      "\u001b[37m#         feature_group.create(\u001b[39;49;00m\n",
      "\u001b[37m#             s3_uri=f\"s3://{bucket}/{prefix}\",\u001b[39;49;00m\n",
      "\u001b[37m#             record_identifier_name=record_identifier_feature_name,\u001b[39;49;00m\n",
      "\u001b[37m#             event_time_feature_name=event_time_feature_name,\u001b[39;49;00m\n",
      "\u001b[37m#             role_arn=role,\u001b[39;49;00m\n",
      "\u001b[37m#             enable_online_store=False,\u001b[39;49;00m\n",
      "\u001b[37m#         )\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"Creating Feature Group. Completed.\")\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#         print(\"Waiting for new Feature Group to become available...\")\u001b[39;49;00m\n",
      "\u001b[37m#         wait_for_feature_group_creation_complete(feature_group)\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"Feature Group available.\")\u001b[39;49;00m\n",
      "\u001b[37m#         feature_group.describe()\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     except Exception as e:\u001b[39;49;00m\n",
      "\u001b[37m#         print(\"Exception: {}\".format(e))\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     return feature_group\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, input_ids, input_mask, segment_ids, label_id, review_id, date, label):\n",
      "        \u001b[37m#               review_body):\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\n",
      "        \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\n",
      "        \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\n",
      "        \u001b[36mself\u001b[39;49;00m.label_id = label_id\n",
      "        \u001b[36mself\u001b[39;49;00m.review_id = review_id\n",
      "        \u001b[36mself\u001b[39;49;00m.date = date\n",
      "        \u001b[36mself\u001b[39;49;00m.label = label\n",
      "\n",
      "\n",
      "\u001b[37m#        self.review_body = review_body\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, review_id, date, label=\u001b[34mNone\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m          text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\n",
      "\u001b[33m            sequence tasks, only this sequence must be specified.\u001b[39;49;00m\n",
      "\u001b[33m          label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\n",
      "\u001b[33m            specified for train and dev examples, but not for test examples.\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.text = text\n",
      "        \u001b[36mself\u001b[39;49;00m.review_id = review_id\n",
      "        \u001b[36mself\u001b[39;49;00m.date = date\n",
      "        \u001b[36mself\u001b[39;49;00m.label = label\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(the_input, max_seq_length):\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    tokens = tokenizer.tokenize(the_input.text)\n",
      "\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    encode_plus_tokens = tokenizer.encode_plus(\n",
      "        the_input.text,\n",
      "        padding=\u001b[33m'\u001b[39;49;00m\u001b[33mmax_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "        max_length=max_seq_length,\n",
      "        truncation=\u001b[34mTrue\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "    \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\n",
      "    input_ids = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.\u001b[39;49;00m\n",
      "    input_mask = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Segment ids are always 0 for single-sequence tasks such as text classification.  1 is used for two-sequence tasks such as question/answer and next sentence prediction.\u001b[39;49;00m\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * max_seq_length\n",
      "\n",
      "    \u001b[37m# Label for each training row (`star_rating` 1 through 5)\u001b[39;49;00m\n",
      "    label_id = label_map[the_input.label]\n",
      "\n",
      "    features = InputFeatures(\n",
      "        input_ids=input_ids,\n",
      "        input_mask=input_mask,\n",
      "        segment_ids=segment_ids,\n",
      "        label_id=label_id,\n",
      "        review_id=the_input.review_id,\n",
      "        date=the_input.date,\n",
      "        label=the_input.label,\n",
      "    )\n",
      "    \u001b[37m#        review_body=the_input.text)\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m#     print('**input_ids**\\n{}\\n'.format(features.input_ids))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**input_mask**\\n{}\\n'.format(features.input_mask))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**label_id**\\n{}\\n'.format(features.label_id))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**review_id**\\n{}\\n'.format(features.review_id))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**date**\\n{}\\n'.format(features.date))\u001b[39;49;00m\n",
      "    \u001b[37m#     print('**label**\\n{}\\n'.format(features.label))\u001b[39;49;00m\n",
      "    \u001b[37m#    print('**review_body**\\n{}\\n'.format(features.review_body))\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m features\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_inputs_to_tfrecord\u001b[39;49;00m(inputs, output_file, max_seq_length):\n",
      "    \u001b[33m\"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\u001b[39;49;00m\n",
      "\n",
      "    records = []\n",
      "\n",
      "    tf_record_writer = tf.io.TFRecordWriter(output_file)\n",
      "\n",
      "    \u001b[34mfor\u001b[39;49;00m (input_idx, the_input) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(inputs):\n",
      "        \u001b[34mif\u001b[39;49;00m input_idx % \u001b[34m10000\u001b[39;49;00m == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mWriting input \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_idx, \u001b[36mlen\u001b[39;49;00m(inputs)))\n",
      "\n",
      "        features = convert_input(the_input, max_seq_length)\n",
      "\n",
      "        all_features = collections.OrderedDict()\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_ids))\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.input_mask))\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=features.segment_ids))\n",
      "        all_features[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = tf.train.Feature(int64_list=tf.train.Int64List(value=[features.label_id]))\n",
      "\n",
      "        tf_record = tf.train.Example(features=tf.train.Features(feature=all_features))\n",
      "        tf_record_writer.write(tf_record.SerializeToString())\n",
      "\n",
      "        records.append(\n",
      "            {  \u001b[37m#'tf_record': tf_record.SerializeToString(),\u001b[39;49;00m\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.input_ids,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.input_mask,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.segment_ids,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.label_id,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: the_input.review_id,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: the_input.date,\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: features.label,\n",
      "                \u001b[37m#                        'review_body': features.review_body\u001b[39;49;00m\n",
      "            }\n",
      "        )\n",
      "\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\n",
      "        \u001b[37m####### TODO:  REMOVE THIS BREAK #######\u001b[39;49;00m\n",
      "        \u001b[37m#####################################\u001b[39;49;00m\n",
      "        \u001b[37m# break\u001b[39;49;00m\n",
      "\n",
      "    tf_record_writer.close()\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m records\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\n",
      "    resconfig = {}\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\n",
      "            resconfig = json.load(cfgfile)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mpass\u001b[39;49;00m  \u001b[37m# Ignore\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m\"\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=list_arg,\n",
      "        default=resconfig.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]),\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=resconfig.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--train-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.90\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--validation-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-split-percentage\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\n",
      "        default=\u001b[34m0.05\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--balance-dataset\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--feature-store-offline-prefix\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--feature-group-name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_transform_tsv_to_tfrecord\u001b[39;49;00m(file, max_seq_length, balance_dataset, prefix, feature_group_name):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfile \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(file))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(max_seq_length))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mbalance_dataset \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(balance_dataset))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mprefix \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(prefix))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfeature_group_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(feature_group_name))\n",
      "\n",
      "    \u001b[37m# need to re-load since we can't pass feature_group object in _partial functions for some reason\u001b[39;49;00m\n",
      "\u001b[37m#    feature_group = create_or_load_feature_group(prefix, feature_group_name)\u001b[39;49;00m\n",
      "\n",
      "    filename_without_extension = Path(Path(file).stem).stem\n",
      "\n",
      "    df = pd.read_csv(file, delimiter=\u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, quoting=csv.QUOTE_NONE, compression=\u001b[33m\"\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    df.isna().values.any()\n",
      "    df = df.dropna()\n",
      "    df = df.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df.shape))\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m balance_dataset:\n",
      "        \u001b[37m# Balance the dataset down to the minority class\u001b[39;49;00m\n",
      "        df_grouped_by = df.groupby([\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]) \n",
      "        df_balanced = df_grouped_by.apply(\u001b[34mlambda\u001b[39;49;00m x: x.sample(df_grouped_by.size().min()).reset_index(drop=\u001b[34mTrue\u001b[39;49;00m))\n",
      "\n",
      "        df_balanced = df_balanced.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of balanced dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_balanced.shape))\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(df_balanced[\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].head(\u001b[34m100\u001b[39;49;00m))\n",
      "\n",
      "        df = df_balanced\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of dataframe before splitting \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df.shape))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.train_split_percentage))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.validation_split_percentage))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest split percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.test_split_percentage))\n",
      "\n",
      "    holdout_percentage = \u001b[34m1.00\u001b[39;49;00m - args.train_split_percentage\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mholdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(holdout_percentage))\n",
      "    \n",
      "    df_train, df_holdout = train_test_split(df, test_size=holdout_percentage, stratify=df[\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest holdout percentage \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_holdout_percentage))\n",
      "    \n",
      "    df_validation, df_test = train_test_split(\n",
      "        df_holdout, test_size=test_holdout_percentage, stratify=df_holdout[\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    df_train = df_train.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    df_validation = df_validation.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    df_test = df_test.reset_index(drop=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of train dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_train.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of validation dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_validation.shape))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of test dataframe \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(df_test.shape))\n",
      "\n",
      "    timestamp = datetime.now().strftime(\u001b[33m\"\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mY-\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mm-\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33mT\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mH:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mM:\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33mSZ\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(timestamp)\n",
      "\n",
      "    train_inputs = df_train.apply(\n",
      "        \u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp\n",
      "        ),\n",
      "        axis=\u001b[34m1\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    validation_inputs = df_validation.apply(\n",
      "        \u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp\n",
      "        ),\n",
      "        axis=\u001b[34m1\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    test_inputs = df_test.apply(\n",
      "        \u001b[34mlambda\u001b[39;49;00m x: Input(\n",
      "            label=x[LABEL_COLUMN], text=x[REVIEW_BODY_COLUMN], review_id=x[REVIEW_ID_COLUMN], date=timestamp\n",
      "        ),\n",
      "        axis=\u001b[34m1\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    \u001b[37m# We don't have to worry about these details.  The Transformers tokenizer does this for us.\u001b[39;49;00m\n",
      "    \u001b[37m#\u001b[39;49;00m\n",
      "    train_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    validation_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    test_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "\n",
      "    \u001b[37m# Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\u001b[39;49;00m\n",
      "    train_records = transform_inputs_to_tfrecord(\n",
      "        train_inputs,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_data, args.current_host, filename_without_extension),\n",
      "        max_seq_length,\n",
      "    )\n",
      "\n",
      "    validation_records = transform_inputs_to_tfrecord(\n",
      "        validation_inputs,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_data, args.current_host, filename_without_extension),\n",
      "        max_seq_length,\n",
      "    )\n",
      "\n",
      "    test_records = transform_inputs_to_tfrecord(\n",
      "        test_inputs,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/part-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_data, args.current_host, filename_without_extension),\n",
      "        max_seq_length,\n",
      "    )\n",
      "\n",
      "    df_train_records = pd.DataFrame.from_dict(train_records)\n",
      "    df_train_records[\u001b[33m\"\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    df_train_records.head()\n",
      "\n",
      "    df_validation_records = pd.DataFrame.from_dict(validation_records)\n",
      "    df_validation_records[\u001b[33m\"\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    df_validation_records.head()\n",
      "\n",
      "    df_test_records = pd.DataFrame.from_dict(test_records)\n",
      "    df_test_records[\u001b[33m\"\u001b[39;49;00m\u001b[33msplit_type\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    df_test_records.head()\n",
      "\n",
      "    \u001b[37m# Add record to feature store\u001b[39;49;00m\n",
      "\u001b[37m#     df_fs_train_records = cast_object_to_string(df_train_records)\u001b[39;49;00m\n",
      "\u001b[37m#     df_fs_validation_records = cast_object_to_string(df_validation_records)\u001b[39;49;00m\n",
      "\u001b[37m#     df_fs_test_records = cast_object_to_string(df_test_records)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     print(\"Ingesting features...\")\u001b[39;49;00m\n",
      "\u001b[37m#     feature_group.ingest(data_frame=df_fs_train_records, max_workers=3, wait=True)\u001b[39;49;00m\n",
      "\u001b[37m#     feature_group.ingest(data_frame=df_fs_validation_records, max_workers=3, wait=True)\u001b[39;49;00m\n",
      "\u001b[37m#     feature_group.ingest(data_frame=df_fs_test_records, max_workers=3, wait=True)\u001b[39;49;00m\n",
      "    \n",
      "\u001b[37m#     offline_store_status = None\u001b[39;49;00m\n",
      "\u001b[37m#     while offline_store_status != 'Active':\u001b[39;49;00m\n",
      "\u001b[37m#         try:\u001b[39;49;00m\n",
      "\u001b[37m#             offline_store_status = feature_group.describe()['OfflineStoreStatus']['Status']\u001b[39;49;00m\n",
      "\u001b[37m#         except:\u001b[39;49;00m\n",
      "\u001b[37m#             pass\u001b[39;49;00m\n",
      "\u001b[37m#         print('Offline store status: {}'.format(offline_store_status))    \u001b[39;49;00m\n",
      "\u001b[37m#     print('...features ingested!')\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.current_host))\n",
      "\n",
      "\u001b[37m#     feature_group = create_or_load_feature_group(\u001b[39;49;00m\n",
      "\u001b[37m#         prefix=args.feature_store_offline_prefix, feature_group_name=args.feature_group_name\u001b[39;49;00m\n",
      "\u001b[37m#     )\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     feature_group.describe()\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     print(feature_group.as_hive_ddl())\u001b[39;49;00m\n",
      "\n",
      "    train_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    validation_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "    test_data = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/bert/test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data)\n",
      "\n",
      "    transform_tsv_to_tfrecord = functools.partial(\n",
      "        _transform_tsv_to_tfrecord,\n",
      "        max_seq_length=args.max_seq_length,\n",
      "        balance_dataset=args.balance_dataset,\n",
      "        prefix=args.feature_store_offline_prefix,\n",
      "        feature_group_name=args.feature_group_name,\n",
      "    )\n",
      "\n",
      "    input_files = glob.glob(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/*.tsv.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_data))\n",
      "\n",
      "    num_cpus = multiprocessing.cpu_count()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_cpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(num_cpus))\n",
      "\n",
      "    p = multiprocessing.Pool(num_cpus)\n",
      "    p.map(transform_tsv_to_tfrecord, input_files)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data))\n",
      "    dirs_output = os.listdir(args.output_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_data))\n",
      "    dirs_output = os.listdir(train_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_data))\n",
      "    dirs_output = os.listdir(validation_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_data))\n",
      "    dirs_output = os.listdir(test_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m dirs_output:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "\u001b[37m#     offline_store_contents = None\u001b[39;49;00m\n",
      "\u001b[37m#     while offline_store_contents is None:\u001b[39;49;00m\n",
      "\u001b[37m#         objects_in_bucket = s3.list_objects(Bucket=bucket, Prefix=args.feature_store_offline_prefix)\u001b[39;49;00m\n",
      "\u001b[37m#         if \"Contents\" in objects_in_bucket and len(objects_in_bucket[\"Contents\"]) > 1:\u001b[39;49;00m\n",
      "\u001b[37m#             offline_store_contents = objects_in_bucket[\"Contents\"]\u001b[39;49;00m\n",
      "\u001b[37m#         else:\u001b[39;49;00m\n",
      "\u001b[37m#             print(\"Waiting for data in offline store...\\n\")\u001b[39;49;00m\n",
      "\u001b[37m#             sleep(60)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     print(\"Data available.\")\u001b[39;49;00m\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args = parse_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\n",
      "\n",
      "    process(args)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./preprocess-scikit-text-to-bert-feature-store.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of an `SKLearnProcessor` processor and we use that in our `ProcessingStep`.\n",
    "\n",
    "We also specify the `framework_version` we will use throughout.\n",
    "\n",
    "Note the `processing_instance_type` and `processing_instance_count` parameters that used by the processor instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={\"AWS_DEFAULT_REGION\": region},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingStep(name='Processing', step_type=<StepTypeEnum.PROCESSING: 'Processing'>)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processing_inputs = [\n",
    "    ProcessingInput(\n",
    "        input_name=\"raw-input-data\",\n",
    "        source=input_data,\n",
    "        destination=\"/opt/ml/processing/input/data/\",\n",
    "        s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "    )\n",
    "]\n",
    "\n",
    "processing_outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name=\"bert-train\",\n",
    "        s3_upload_mode=\"EndOfJob\",\n",
    "        source=\"/opt/ml/processing/output/bert/train\",\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        output_name=\"bert-validation\",\n",
    "        s3_upload_mode=\"EndOfJob\",\n",
    "        source=\"/opt/ml/processing/output/bert/validation\",\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        output_name=\"bert-test\",\n",
    "        s3_upload_mode=\"EndOfJob\",\n",
    "        source=\"/opt/ml/processing/output/bert/test\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "processing_step = ProcessingStep(\n",
    "    name=\"Processing\",\n",
    "    code=\"preprocess-scikit-text-to-bert-feature-store.py\",\n",
    "    processor=processor,\n",
    "    inputs=processing_inputs,\n",
    "    outputs=processing_outputs,\n",
    "    job_arguments=[\n",
    "        \"--train-split-percentage\",\n",
    "        str(train_split_percentage.default_value),\n",
    "        \"--validation-split-percentage\",\n",
    "        str(validation_split_percentage.default_value),\n",
    "        \"--test-split-percentage\",\n",
    "        str(test_split_percentage.default_value),\n",
    "        \"--max-seq-length\",\n",
    "        str(max_seq_length.default_value),\n",
    "        \"--balance-dataset\",\n",
    "        str(balance_dataset.default_value),\n",
    "        \"--feature-store-offline-prefix\",\n",
    "        str(feature_store_offline_prefix.default_value),\n",
    "        \"--feature-group-name\",\n",
    "        str(feature_group_name.default_value),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(processing_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the processor instance to construct a `ProcessingStep`, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to a processor instance's `run` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "Note the `input_data` parameters passed into `ProcessingStep` as the input data of the step itself. This input data will be used by the processor instance when it is run.\n",
    "\n",
    "Also, take note the `\"bert-train\"`, `\"bert-validation\"` and `\"bert-test\"` named channels specified in the output configuration for the processing job. Such step `Properties` can be used in subsequent steps and will resolve to their runtime values at execution. In particular, we'll call out this usage when we define our training step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Training Step to Train a Model](img/pipeline-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = ParameterString(name=\"TrainInstanceType\", default_value=\"ml.c5.9xlarge\")\n",
    "\n",
    "train_instance_count = ParameterInteger(name=\"TrainInstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Hyper-Parameters\n",
    "Note that `max_seq_length` is re-used from the processing hyper-parameters above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = ParameterInteger(name=\"Epochs\", default_value=1)\n",
    "\n",
    "learning_rate = ParameterFloat(name=\"LearningRate\", default_value=0.00001)\n",
    "\n",
    "epsilon = ParameterFloat(name=\"Epsilon\", default_value=0.00000001)\n",
    "\n",
    "train_batch_size = ParameterInteger(name=\"TrainBatchSize\", default_value=128)\n",
    "\n",
    "validation_batch_size = ParameterInteger(name=\"ValidationBatchSize\", default_value=128)\n",
    "\n",
    "test_batch_size = ParameterInteger(name=\"TestBatchSize\", default_value=128)\n",
    "\n",
    "train_steps_per_epoch = ParameterInteger(name=\"TrainStepsPerEpoch\", default_value=50)\n",
    "\n",
    "validation_steps = ParameterInteger(name=\"ValidationSteps\", default_value=50)\n",
    "\n",
    "test_steps = ParameterInteger(name=\"TestSteps\", default_value=50)\n",
    "\n",
    "train_volume_size = ParameterInteger(name=\"TrainVolumeSize\", default_value=1024)\n",
    "\n",
    "use_xla = ParameterString(\n",
    "    name=\"UseXLA\",\n",
    "    default_value=\"True\",\n",
    ")\n",
    "\n",
    "use_amp = ParameterString(\n",
    "    name=\"UseAMP\",\n",
    "    default_value=\"True\",\n",
    ")\n",
    "\n",
    "freeze_bert_layer = ParameterString(\n",
    "    name=\"FreezeBERTLayer\",\n",
    "    default_value=\"False\",\n",
    ")\n",
    "\n",
    "enable_sagemaker_debugger = ParameterString(\n",
    "    name=\"EnableSageMakerDebugger\",\n",
    "    default_value=\"False\",\n",
    ")\n",
    "\n",
    "enable_checkpointing = ParameterString(\n",
    "    name=\"EnableCheckpointing\",\n",
    "    default_value=\"False\",\n",
    ")\n",
    "\n",
    "enable_tensorboard = ParameterString(\n",
    "    name=\"EnableTensorboard\",\n",
    "    default_value=\"False\",\n",
    ")\n",
    "\n",
    "input_mode = ParameterString(\n",
    "    name=\"InputMode\",\n",
    "    default_value=\"File\",\n",
    ")\n",
    "\n",
    "run_validation = ParameterString(\n",
    "    name=\"RunValidation\",\n",
    "    default_value=\"True\",\n",
    ")\n",
    "\n",
    "run_test = ParameterString(\n",
    "    name=\"RunTest\",\n",
    "    default_value=\"False\",\n",
    ")\n",
    "\n",
    "run_sample_predictions = ParameterString(\n",
    "    name=\"RunSamplePredictions\",\n",
    "    default_value=\"False\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics To Track Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"train:accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"validation:loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n",
    "    {\"Name\": \"validation:accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m# subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\u001b[37m# subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.1.0.1.0.0'])\u001b[39;49;00m\n",
      "\u001b[37m# subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'smdebug==0.9.3'])\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mscikit-learn==0.23.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mmatplotlib==3.2.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFDistilBertForSequenceClassification\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcallbacks\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ModelCheckpoint\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_model\n",
      "\n",
      "\n",
      "CLASSES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mselect_data_and_label_from_record\u001b[39;49;00m(record):\n",
      "    x = {\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: record[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: record[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: record[\u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]}\n",
      "\n",
      "    y = record[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m (x, y)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfile_based_input_dataset_builder\u001b[39;49;00m(\n",
      "    channel,\n",
      "    input_filenames,\n",
      "    pipe_mode,\n",
      "    is_training,\n",
      "    drop_remainder,\n",
      "    batch_size,\n",
      "    epochs,\n",
      "    steps_per_epoch,\n",
      "    max_seq_length,\n",
      "):\n",
      "\n",
      "    \u001b[37m# For training, we want a lot of parallel reading and shuffling.\u001b[39;49;00m\n",
      "    \u001b[37m# For eval, we want no shuffling and parallel reading doesn't matter.\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m pipe_mode:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Using pipe_mode with channel \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(channel))\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_tensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PipeModeDataset\n",
      "\n",
      "        dataset = PipeModeDataset(channel=channel, record_format=\u001b[33m\"\u001b[39;49;00m\u001b[33mTFRecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Using input_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(input_filenames))\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\n",
      "\n",
      "    dataset = dataset.repeat(epochs * steps_per_epoch * \u001b[34m100\u001b[39;49;00m)\n",
      "    \u001b[37m#    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\u001b[39;49;00m\n",
      "\n",
      "    name_to_features = {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\n",
      "    }\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_decode_record\u001b[39;49;00m(record, name_to_features):\n",
      "        \u001b[33m\"\"\"Decodes a record to a TensorFlow example.\"\"\"\u001b[39;49;00m\n",
      "        record = tf.io.parse_single_example(record, name_to_features)\n",
      "        \u001b[34mreturn\u001b[39;49;00m record\n",
      "\n",
      "    dataset = dataset.apply(\n",
      "        tf.data.experimental.map_and_batch(\n",
      "            \u001b[34mlambda\u001b[39;49;00m record: _decode_record(record, name_to_features),\n",
      "            batch_size=batch_size,\n",
      "            drop_remainder=drop_remainder,\n",
      "            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
      "        )\n",
      "    )\n",
      "\n",
      "    \u001b[37m#    dataset.cache()\u001b[39;49;00m\n",
      "\n",
      "    dataset = dataset.shuffle(buffer_size=\u001b[34m1000\u001b[39;49;00m, reshuffle_each_iteration=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    row_count = \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m**************** \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m *****************\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(channel))\n",
      "    \u001b[34mfor\u001b[39;49;00m row \u001b[35min\u001b[39;49;00m dataset.as_numpy_iterator():\n",
      "        \u001b[36mprint\u001b[39;49;00m(row)\n",
      "        \u001b[34mif\u001b[39;49;00m row_count == \u001b[34m5\u001b[39;49;00m:\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "        row_count = row_count + \u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m dataset\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_checkpoint_model\u001b[39;49;00m(checkpoint_path):\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\n",
      "    glob_pattern = os.path.join(checkpoint_path, \u001b[33m\"\u001b[39;49;00m\u001b[33m*.h5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mglob pattern \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(glob_pattern))\n",
      "\n",
      "    list_of_checkpoint_files = glob.glob(glob_pattern)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mList of checkpoint files \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(list_of_checkpoint_files))\n",
      "\n",
      "    latest_checkpoint_file = \u001b[36mmax\u001b[39;49;00m(list_of_checkpoint_files)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLatest checkpoint file \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(latest_checkpoint_file))\n",
      "\n",
      "    initial_epoch_number_str = latest_checkpoint_file.rsplit(\u001b[33m\"\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)[-\u001b[34m1\u001b[39;49;00m].split(\u001b[33m\"\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)[\u001b[34m0\u001b[39;49;00m]\n",
      "    initial_epoch_number = \u001b[36mint\u001b[39;49;00m(initial_epoch_number_str)\n",
      "\n",
      "    loaded_model = TFDistilBertForSequenceClassification.from_pretrained(latest_checkpoint_file, config=config)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mloaded_model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(loaded_model))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33minitial_epoch_number \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(initial_epoch_number))\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m loaded_model, initial_epoch_number\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--checkpoint_base_path\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--use_xla\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--use_amp\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_seq_length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m128\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m256\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.00003\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epsilon\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.00000001\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_steps_per_epoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--validation_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--freeze_bert_layer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--enable_sagemaker_debugger\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--run_validation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--run_test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--run_sample_predictions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--enable_tensorboard\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--enable_checkpointing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])  \u001b[37m# This is unused\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# This points to the S3 location - this should not be used by our code\u001b[39;49;00m\n",
      "    \u001b[37m# We should use /opt/ml/model/ instead\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument('--model_dir',\u001b[39;49;00m\n",
      "    \u001b[37m#                     type=str,\u001b[39;49;00m\n",
      "    \u001b[37m#                     default=os.environ['SM_MODEL_DIR'])\u001b[39;49;00m\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mArgs:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "\n",
      "    env_var = os.environ\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment Variables:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    pprint.pprint(\u001b[36mdict\u001b[39;49;00m(env_var), width=\u001b[34m1\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(env_var[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\n",
      "    sm_training_env_json = json.loads(env_var[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_TRAINING_ENV\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    is_master = sm_training_env_json[\u001b[33m\"\u001b[39;49;00m\u001b[33mis_master\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mis_master \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(is_master))\n",
      "\n",
      "    train_data = args.train_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_data))\n",
      "    validation_data = args.validation_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_data))\n",
      "    test_data = args.test_data\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_data \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_data))\n",
      "    local_model_dir = os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    output_dir = args.output_dir\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33moutput_dir \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(output_dir))\n",
      "    hosts = args.hosts\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mhosts \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(hosts))\n",
      "    current_host = args.current_host\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcurrent_host \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(current_host))\n",
      "    num_gpus = args.num_gpus\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_gpus \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(num_gpus))\n",
      "    job_name = os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSAGEMAKER_JOB_NAME\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mjob_name \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(job_name))\n",
      "    use_xla = args.use_xla\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33muse_xla \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(use_xla))\n",
      "    use_amp = args.use_amp\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33muse_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(use_amp))\n",
      "    max_seq_length = args.max_seq_length\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmax_seq_length \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(max_seq_length))\n",
      "    train_batch_size = args.train_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_batch_size))\n",
      "    validation_batch_size = args.validation_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_batch_size))\n",
      "    test_batch_size = args.test_batch_size\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_batch_size \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_batch_size))\n",
      "    epochs = args.epochs\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mepochs \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epochs))\n",
      "    learning_rate = args.learning_rate\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning_rate \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(learning_rate))\n",
      "    epsilon = args.epsilon\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mepsilon \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epsilon))\n",
      "    train_steps_per_epoch = args.train_steps_per_epoch\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_steps_per_epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_steps_per_epoch))\n",
      "    validation_steps = args.validation_steps\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_steps))\n",
      "    test_steps = args.test_steps\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_steps \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_steps))\n",
      "    freeze_bert_layer = args.freeze_bert_layer\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mfreeze_bert_layer \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(freeze_bert_layer))\n",
      "    enable_sagemaker_debugger = args.enable_sagemaker_debugger\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33menable_sagemaker_debugger \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(enable_sagemaker_debugger))\n",
      "    run_validation = args.run_validation\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mrun_validation \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(run_validation))\n",
      "    run_test = args.run_test\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mrun_test \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(run_test))\n",
      "    run_sample_predictions = args.run_sample_predictions\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mrun_sample_predictions \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(run_sample_predictions))\n",
      "    enable_tensorboard = args.enable_tensorboard\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33menable_tensorboard \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(enable_tensorboard))\n",
      "    enable_checkpointing = args.enable_checkpointing\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33menable_checkpointing \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(enable_checkpointing))\n",
      "\n",
      "    checkpoint_base_path = args.checkpoint_base_path\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcheckpoint_base_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(checkpoint_base_path))\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m is_master:\n",
      "        checkpoint_path = checkpoint_base_path\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        checkpoint_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/checkpoints\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mcheckpoint_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(checkpoint_path))\n",
      "\n",
      "    \u001b[37m# Determine if PipeMode is enabled\u001b[39;49;00m\n",
      "    pipe_mode_str = os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_INPUT_DATA_CONFIG\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    pipe_mode = pipe_mode_str.find(\u001b[33m\"\u001b[39;49;00m\u001b[33mPipe\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) >= \u001b[34m0\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing pipe_mode: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(pipe_mode))\n",
      "\n",
      "    \u001b[37m# Model Output\u001b[39;49;00m\n",
      "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtransformers/fine-tuned/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# SavedModel Output\u001b[39;49;00m\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtensorflow/saved_model/0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Tensorboard Logs\u001b[39;49;00m\n",
      "    tensorboard_logs_path = os.path.join(local_model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtensorboard/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    os.makedirs(tensorboard_logs_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Commented out due to incompatibility with transformers library (possibly)\u001b[39;49;00m\n",
      "    \u001b[37m# Set the global precision mixed_precision policy to \"mixed_float16\"\u001b[39;49;00m\n",
      "    \u001b[37m#    mixed_precision_policy = 'mixed_float16'\u001b[39;49;00m\n",
      "    \u001b[37m#    print('Mixed precision policy {}'.format(mixed_precision_policy))\u001b[39;49;00m\n",
      "    \u001b[37m#    policy = mixed_precision.Policy(mixed_precision_policy)\u001b[39;49;00m\n",
      "    \u001b[37m#    mixed_precision.set_policy(policy)\u001b[39;49;00m\n",
      "\n",
      "    distributed_strategy = tf.distribute.MirroredStrategy()\n",
      "    \u001b[37m# Comment out when using smdebug as smdebug does not support MultiWorkerMirroredStrategy() as of smdebug 0.8.0\u001b[39;49;00m\n",
      "    \u001b[37m# distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m distributed_strategy.scope():\n",
      "        tf.config.optimizer.set_jit(use_xla)\n",
      "        tf.config.optimizer.set_experimental_options({\u001b[33m\"\u001b[39;49;00m\u001b[33mauto_mixed_precision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: use_amp})\n",
      "\n",
      "        train_data_filenames = glob(os.path.join(train_data, \u001b[33m\"\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(train_data_filenames))\n",
      "        train_dataset = file_based_input_dataset_builder(\n",
      "            channel=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            input_filenames=train_data_filenames,\n",
      "            pipe_mode=pipe_mode,\n",
      "            is_training=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "            batch_size=train_batch_size,\n",
      "            epochs=epochs,\n",
      "            steps_per_epoch=train_steps_per_epoch,\n",
      "            max_seq_length=max_seq_length,\n",
      "        ).map(select_data_and_label_from_record)\n",
      "\n",
      "        tokenizer = \u001b[34mNone\u001b[39;49;00m\n",
      "        config = \u001b[34mNone\u001b[39;49;00m\n",
      "        model = \u001b[34mNone\u001b[39;49;00m\n",
      "        transformer_model = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "        \u001b[37m# This is required when launching many instances at once...  the urllib request seems to get denied periodically\u001b[39;49;00m\n",
      "        successful_download = \u001b[34mFalse\u001b[39;49;00m\n",
      "        retries = \u001b[34m0\u001b[39;49;00m\n",
      "        \u001b[34mwhile\u001b[39;49;00m retries < \u001b[34m5\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m successful_download:\n",
      "            \u001b[34mtry\u001b[39;49;00m:\n",
      "                tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "                config = DistilBertConfig.from_pretrained(\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                    num_labels=\u001b[36mlen\u001b[39;49;00m(CLASSES),\n",
      "                    id2label={\u001b[34m0\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m: \u001b[34m5\u001b[39;49;00m},\n",
      "                    label2id={\u001b[34m1\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m},\n",
      "                )\n",
      "\n",
      "                transformer_model = TFDistilBertForSequenceClassification.from_pretrained(\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, config=config\n",
      "                )\n",
      "\n",
      "                input_ids = tf.keras.layers.Input(shape=(max_seq_length,), name=\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "                input_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mint32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "                embedding_layer = transformer_model.distilbert(input_ids, attention_mask=input_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "                X = tf.keras.layers.Bidirectional(\n",
      "                    tf.keras.layers.LSTM(\u001b[34m50\u001b[39;49;00m, return_sequences=\u001b[34mTrue\u001b[39;49;00m, dropout=\u001b[34m0.1\u001b[39;49;00m, recurrent_dropout=\u001b[34m0.1\u001b[39;49;00m)\n",
      "                )(embedding_layer)\n",
      "                X = tf.keras.layers.GlobalMaxPool1D()(X)\n",
      "                X = tf.keras.layers.Dense(\u001b[34m50\u001b[39;49;00m, activation=\u001b[33m\"\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)(X)\n",
      "                X = tf.keras.layers.Dropout(\u001b[34m0.2\u001b[39;49;00m)(X)\n",
      "                X = tf.keras.layers.Dense(\u001b[36mlen\u001b[39;49;00m(CLASSES), activation=\u001b[33m\"\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)(X)\n",
      "\n",
      "                model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=X)\n",
      "\n",
      "                \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[:\u001b[34m3\u001b[39;49;00m]:\n",
      "                    layer.trainable = \u001b[35mnot\u001b[39;49;00m freeze_bert_layer\n",
      "\n",
      "                successful_download = \u001b[34mTrue\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSucessfully downloaded after \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m retries.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(retries))\n",
      "            \u001b[34mexcept\u001b[39;49;00m:\n",
      "                retries = retries + \u001b[34m1\u001b[39;49;00m\n",
      "                random_sleep = random.randint(\u001b[34m1\u001b[39;49;00m, \u001b[34m30\u001b[39;49;00m)\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mRetry #\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m.  Sleeping for \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m seconds\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(retries, random_sleep))\n",
      "                time.sleep(random_sleep)\n",
      "\n",
      "        callbacks = []\n",
      "\n",
      "        initial_epoch_number = \u001b[34m0\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m enable_checkpointing:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Checkpoint enabled *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "            os.makedirs(checkpoint_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "            \u001b[34mif\u001b[39;49;00m os.listdir(checkpoint_path):\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Found checkpoint *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "                \u001b[36mprint\u001b[39;49;00m(checkpoint_path)\n",
      "                model, initial_epoch_number = load_checkpoint_model(checkpoint_path)\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Using checkpoint model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model))\n",
      "\n",
      "            checkpoint_callback = ModelCheckpoint(\n",
      "                filepath=os.path.join(checkpoint_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mtf_model_\u001b[39;49;00m\u001b[33m{epoch:05d}\u001b[39;49;00m\u001b[33m.h5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "                save_weights_only=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                verbose=\u001b[34m1\u001b[39;49;00m,\n",
      "                monitor=\u001b[33m\"\u001b[39;49;00m\u001b[33mval_accuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            )\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** CHECKPOINT CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(checkpoint_callback))\n",
      "            callbacks.append(checkpoint_callback)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m tokenizer \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m model \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m config:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mNot properly initialized...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m** use_amp \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(use_amp))\n",
      "        \u001b[34mif\u001b[39;49;00m use_amp:\n",
      "            \u001b[37m# loss scaling is currently required when using mixed precision\u001b[39;49;00m\n",
      "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, \u001b[33m\"\u001b[39;49;00m\u001b[33mdynamic\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33menable_sagemaker_debugger \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(enable_sagemaker_debugger))\n",
      "        \u001b[34mif\u001b[39;49;00m enable_sagemaker_debugger:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** DEBUGGING ***\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msmdebug\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36msmd\u001b[39;49;00m\n",
      "\n",
      "            \u001b[37m# This assumes that we specified debugger_hook_config\u001b[39;49;00m\n",
      "            debugger_callback = smd.KerasHook.create_from_json_file()\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** DEBUGGER CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(debugger_callback))\n",
      "            callbacks.append(debugger_callback)\n",
      "            optimizer = debugger_callback.wrap_optimizer(optimizer)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m enable_tensorboard:\n",
      "            tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_path)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** TENSORBOARD CALLBACK \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(tensorboard_callback))\n",
      "            callbacks.append(tensorboard_callback)\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m*** OPTIMIZER \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m ***\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(optimizer))\n",
      "\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        metric = tf.keras.metrics.SparseCategoricalAccuracy(\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCompiled model \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model))\n",
      "        \u001b[37m#        model.layers[0].trainable = not freeze_bert_layer\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(model.summary())\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m run_validation:\n",
      "            validation_data_filenames = glob(os.path.join(validation_data, \u001b[33m\"\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(validation_data_filenames))\n",
      "            validation_dataset = file_based_input_dataset_builder(\n",
      "                channel=\u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                input_filenames=validation_data_filenames,\n",
      "                pipe_mode=pipe_mode,\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                batch_size=validation_batch_size,\n",
      "                epochs=epochs,\n",
      "                steps_per_epoch=validation_steps,\n",
      "                max_seq_length=max_seq_length,\n",
      "            ).map(select_data_and_label_from_record)\n",
      "\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting Training and Validation...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            validation_dataset = validation_dataset.take(validation_steps)\n",
      "            train_and_validation_history = model.fit(\n",
      "                train_dataset,\n",
      "                shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                epochs=epochs,\n",
      "                initial_epoch=initial_epoch_number,\n",
      "                steps_per_epoch=train_steps_per_epoch,\n",
      "                validation_data=validation_dataset,\n",
      "                validation_steps=validation_steps,\n",
      "                callbacks=callbacks,\n",
      "            )\n",
      "            \u001b[36mprint\u001b[39;49;00m(train_and_validation_history)\n",
      "        \u001b[34melse\u001b[39;49;00m:  \u001b[37m# Not running validation\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting Training (Without Validation)...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            train_history = model.fit(\n",
      "                train_dataset,\n",
      "                shuffle=\u001b[34mTrue\u001b[39;49;00m,\n",
      "                epochs=epochs,\n",
      "                initial_epoch=initial_epoch_number,\n",
      "                steps_per_epoch=train_steps_per_epoch,\n",
      "                callbacks=callbacks,\n",
      "            )\n",
      "            \u001b[36mprint\u001b[39;49;00m(train_history)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m run_test:\n",
      "            test_data_filenames = glob(os.path.join(test_data, \u001b[33m\"\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_data_filenames \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_data_filenames))\n",
      "            test_dataset = file_based_input_dataset_builder(\n",
      "                channel=\u001b[33m\"\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                input_filenames=test_data_filenames,\n",
      "                pipe_mode=pipe_mode,\n",
      "                is_training=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                drop_remainder=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                batch_size=test_batch_size,\n",
      "                epochs=epochs,\n",
      "                steps_per_epoch=test_steps,\n",
      "                max_seq_length=max_seq_length,\n",
      "            ).map(select_data_and_label_from_record)\n",
      "\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting test...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            test_history = model.evaluate(test_dataset, steps=test_steps, callbacks=callbacks)\n",
      "\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTest history \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_history))\n",
      "\n",
      "        \u001b[37m# Save the Fine-Yuned Transformers Model as a New \"Pre-Trained\" Model\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtransformer_fine_tuned_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(transformer_fine_tuned_model_path))\n",
      "        transformer_model.save_pretrained(transformer_fine_tuned_model_path)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mModel inputs after save_pretrained: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model.inputs))\n",
      "\n",
      "        \u001b[37m# Save the TensorFlow SavedModel for Serving Predictions\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mtensorflow_saved_model_path \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(tensorflow_saved_model_path))\n",
      "        model.save(tensorflow_saved_model_path, include_optimizer=\u001b[34mFalse\u001b[39;49;00m, overwrite=\u001b[34mTrue\u001b[39;49;00m, save_format=\u001b[33m\"\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# Copy inference.py and requirements.txt to the code/ directory\u001b[39;49;00m\n",
      "        \u001b[37m#   Note: This is required for the SageMaker Endpoint to pick them up.\u001b[39;49;00m\n",
      "        \u001b[37m#         This appears to be hard-coded and must be called code/\u001b[39;49;00m\n",
      "        inference_path = os.path.join(local_model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mcode/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCopying inference source files to \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(inference_path))\n",
      "        os.makedirs(inference_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        os.system(\u001b[33m\"\u001b[39;49;00m\u001b[33mcp inference.py \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(inference_path))\n",
      "        \u001b[36mprint\u001b[39;49;00m(glob(inference_path))\n",
      "        \u001b[37m#        os.system('cp requirements.txt {}/code'.format(inference_path))\u001b[39;49;00m\n",
      "\n",
      "        \u001b[37m# Copy test data for the evaluation step\u001b[39;49;00m\n",
      "        os.system(\u001b[33m\"\u001b[39;49;00m\u001b[33mcp -R ./test_data/ \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(local_model_dir))\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m run_sample_predictions:\n",
      "\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mpredict\u001b[39;49;00m(text):\n",
      "            encode_plus_tokens = tokenizer.encode_plus(\n",
      "                text, \n",
      "                padding=\u001b[33m'\u001b[39;49;00m\u001b[33mmax_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "                max_length=max_seq_length, \n",
      "                truncation=\u001b[34mTrue\u001b[39;49;00m, \n",
      "                return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "            )\n",
      "            \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\n",
      "            input_ids = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "            \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.\u001b[39;49;00m\n",
      "            input_mask = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "            outputs = model.predict(x=(input_ids, input_mask))\n",
      "\n",
      "            prediction = [{\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: config.id2label[item.argmax()], \u001b[33m\"\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: item.max().item()} \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m outputs]\n",
      "\n",
      "            \u001b[34mreturn\u001b[39;49;00m prediction[\u001b[34m0\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\n",
      "            \u001b[33m\"\"\"I loved it!  I will recommend this to everyone.\"\"\"\u001b[39;49;00m,\n",
      "            predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m),\n",
      "        )\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\n",
      "            \u001b[33m\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"\u001b[39;49;00m,\n",
      "            predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m),\n",
      "        )\n",
      "\n",
      "        df_test_reviews = pd.read_csv(\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33m./test_data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            delimiter=\u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "            quoting=csv.QUOTE_NONE,\n",
      "            compression=\u001b[33m\"\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        )[[\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]\n",
      "\n",
      "        df_test_reviews = df_test_reviews.sample(n=\u001b[34m100\u001b[39;49;00m)\n",
      "        df_test_reviews.shape\n",
      "        df_test_reviews.head()\n",
      "\n",
      "        y_test = df_test_reviews[\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].map(predict)\n",
      "        y_test\n",
      "\n",
      "        y_actual = df_test_reviews[\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        y_actual\n",
      "\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m classification_report\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(classification_report(y_true=y_test, y_pred=y_actual))\n",
      "\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score\n",
      "\n",
      "        accuracy = accuracy_score(y_true=y_test, y_pred=y_actual)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTest accuracy: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, accuracy)\n",
      "\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mdef\u001b[39;49;00m \u001b[32mplot_conf_mat\u001b[39;49;00m(cm, classes, title, cmap=plt.cm.Greens):\n",
      "            \u001b[36mprint\u001b[39;49;00m(cm)\n",
      "            plt.imshow(cm, interpolation=\u001b[33m\"\u001b[39;49;00m\u001b[33mnearest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, cmap=cmap)\n",
      "            plt.title(title)\n",
      "            plt.colorbar()\n",
      "            tick_marks = np.arange(\u001b[36mlen\u001b[39;49;00m(classes))\n",
      "            plt.xticks(tick_marks, classes, rotation=\u001b[34m45\u001b[39;49;00m)\n",
      "            plt.yticks(tick_marks, classes)\n",
      "\n",
      "            fmt = \u001b[33m\"\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "            thresh = cm.max() / \u001b[34m2.0\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m i, j \u001b[35min\u001b[39;49;00m itertools.product(\u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m0\u001b[39;49;00m]), \u001b[36mrange\u001b[39;49;00m(cm.shape[\u001b[34m1\u001b[39;49;00m])):\n",
      "                plt.text(\n",
      "                    j,\n",
      "                    i,\n",
      "                    \u001b[36mformat\u001b[39;49;00m(cm[i, j], fmt),\n",
      "                    horizontalalignment=\u001b[33m\"\u001b[39;49;00m\u001b[33mcenter\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                    color=\u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m cm[i, j] > thresh \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mblack\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "                )\n",
      "\n",
      "                plt.tight_layout()\n",
      "                plt.ylabel(\u001b[33m\"\u001b[39;49;00m\u001b[33mTrue label\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "                plt.xlabel(\u001b[33m\"\u001b[39;49;00m\u001b[33mPredicted label\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m confusion_matrix\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmatplotlib\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpyplot\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mplt\u001b[39;49;00m\n",
      "\n",
      "        cm = confusion_matrix(y_true=y_test, y_pred=y_actual)\n",
      "\n",
      "        plt.figure()\n",
      "        fig, ax = plt.subplots(figsize=(\u001b[34m10\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m))\n",
      "        plot_conf_mat(cm, classes=[\u001b[33m\"\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m3\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], title=\u001b[33m\"\u001b[39;49;00m\u001b[33mConfusion Matrix\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# Save the confusion matrix\u001b[39;49;00m\n",
      "        plt.show()\n",
      "\n",
      "        \u001b[37m# Model Output\u001b[39;49;00m\n",
      "        metrics_path = os.path.join(local_model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        os.makedirs(metrics_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "        plt.savefig(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/confusion_matrix.png\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(metrics_path))\n",
      "\n",
      "        report_dict = {\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: accuracy,\n",
      "                },\n",
      "            },\n",
      "        }\n",
      "\n",
      "        evaluation_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(metrics_path)\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(evaluation_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "            f.write(json.dumps(report_dict))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Debugger and Profiler\n",
    "Define Debugger Rules as described here:  https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-built-in-rules.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule, ProfilerRule, rule_configs\n",
    "from sagemaker.debugger import DebuggerHookConfig\n",
    "from sagemaker.debugger import ProfilerConfig, FrameworkProfile\n",
    "\n",
    "debugger_hook_config = DebuggerHookConfig(\n",
    "    s3_output_path=\"s3://{}\".format(bucket),\n",
    ")\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    "    framework_profile_params=FrameworkProfile(local_path=\"/opt/ml/output/profiler/\", start_step=5, num_steps=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = [ProfilerRule.sagemaker(rule_configs.ProfilerReport())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Training Step to Train a Model\n",
    "\n",
    "We configure an Estimator and the input dataset. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later.\n",
    "\n",
    "We also specify the model path where the models from training will be saved.\n",
    "\n",
    "Note the `train_instance_type` parameter passed may be also used and passed into other places in the pipeline. In this case, the `train_instance_type` is passed into the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point=\"tf_bert_reviews.py\",\n",
    "    source_dir=\"src\",\n",
    "    role=role,\n",
    "    instance_count=train_instance_count,  # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available\n",
    "    instance_type=train_instance_type,\n",
    "    volume_size=train_volume_size,\n",
    "    py_version=\"py37\",\n",
    "    framework_version=\"2.3.1\",\n",
    "    hyperparameters={\n",
    "        \"epochs\": epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epsilon\": epsilon,\n",
    "        \"train_batch_size\": train_batch_size,\n",
    "        \"validation_batch_size\": validation_batch_size,\n",
    "        \"test_batch_size\": test_batch_size,\n",
    "        \"train_steps_per_epoch\": train_steps_per_epoch,\n",
    "        \"validation_steps\": validation_steps,\n",
    "        \"test_steps\": test_steps,\n",
    "        \"use_xla\": use_xla,\n",
    "        \"use_amp\": use_amp,\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "        \"freeze_bert_layer\": freeze_bert_layer,\n",
    "        \"enable_sagemaker_debugger\": enable_sagemaker_debugger,\n",
    "        \"enable_checkpointing\": enable_checkpointing,\n",
    "        \"enable_tensorboard\": enable_tensorboard,\n",
    "        \"run_validation\": run_validation,\n",
    "        \"run_test\": run_test,\n",
    "        \"run_sample_predictions\": run_sample_predictions,\n",
    "    },\n",
    "    input_mode=input_mode,\n",
    "    metric_definitions=metrics_definitions,\n",
    "    debugger_hook_config=debugger_hook_config,\n",
    "    profiler_config=profiler_config,\n",
    "    rules=rules,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pipeline Step Caching\n",
    "Cache pipeline steps for a duration of time using [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601#Durations) format.  \n",
    "\n",
    "More details on SageMaker Pipeline step caching here:  https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"PT1H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Training Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the estimator instance to construct a `TrainingStep` as well as the `Properties` of the prior `ProcessingStep` used as input in the `TrainingStep` inputs and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to an estimator's `fit` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "In particular, we pass in the `S3Uri` of the `\"train\"`, `\"validation\"` and `\"test\"` output channel to the `TrainingStep`. The `properties` attribute of a Workflow step match the object model of the corresponding response of a describe call. These properties can be referenced as placeholder values and are resolved, or filled in, at runtime. For example, the `ProcessingStep` `properties` attribute matches the object model of the [DescribeProcessingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeProcessingJob.html) response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingStep(name='Train', step_type=<StepTypeEnum.TRAINING: 'Training'>)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "training_step = TrainingStep(\n",
    "    name=\"Train\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\"bert-train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\"bert-validation\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=processing_step.properties.ProcessingOutputConfig.Outputs[\"bert-test\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "\n",
    "print(training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Define a Model Evaluation Step to Evaluate the Trained Model](img/pipeline-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we develop an evaluation script that will be specified in a Processing step that will perform the model evaluation.\n",
    "\n",
    "The evaluation script `evaluation.py` takes the trained model and the test dataset as input, and produces a JSON file containing classification evaluation metrics such as accuracy.\n",
    "\n",
    "After pipeline execution, we will examine the resulting `evaluation.json` for analysis.\n",
    "\n",
    "The evaluation script:\n",
    "\n",
    "* loads in the model\n",
    "* reads in the test data\n",
    "* issues a bunch of predictions against the test data\n",
    "* builds a classification report, including accuracy\n",
    "* saves the evaluation report to the evaluation directory\n",
    "\n",
    "Next, we create an instance of a `ScriptProcessor` processor and we use that in our `ProcessingStep`.\n",
    "\n",
    "Note the `processing_instance_type` parameter passed into the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "evaluation_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    env={\"AWS_DEFAULT_REGION\": region},\n",
    "    max_runtime_in_seconds=7200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mfunctools\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmultiprocessing\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatetime\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m datetime\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#subprocess.check_call([sys.executable, \"-m\", \"conda\", \"install\", \"-c\", \"anaconda\", \"tensorflow==2.3.0\", \"-y\"])\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtensorflow==2.3.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\n",
      "\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mconda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-c\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mconda-forge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mtransformers==3.5.1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-y\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\n",
      "\n",
      "\u001b[37m#subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"matplotlib==3.2.1\"])\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mre\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m confusion_matrix\n",
      "\u001b[37m#import matplotlib.pyplot as plt\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m keras\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m classification_report\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_selection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m train_test_split\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m resample\n",
      "\n",
      "\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "CLASSES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\n",
      "\n",
      "config = DistilBertConfig.from_pretrained(\n",
      "    \u001b[33m\"\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    num_labels=\u001b[36mlen\u001b[39;49;00m(CLASSES),\n",
      "    id2label={\u001b[34m0\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m: \u001b[34m5\u001b[39;49;00m},\n",
      "    label2id={\u001b[34m1\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m},\n",
      ")\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\n",
      "    resconfig = {}\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\n",
      "            resconfig = json.load(cfgfile)\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mpass\u001b[39;49;00m  \u001b[37m# Ignore\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m\"\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=list_arg,\n",
      "        default=resconfig.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]),\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=resconfig.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--input-model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/model\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\n",
      "        default=\u001b[33m\"\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--max-seq-length\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "        default=\u001b[34m64\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict\u001b[39;49;00m(text, model, max_seq_length):\n",
      "    encode_plus_tokens = tokenizer.encode_plus(\n",
      "        text, \n",
      "        padding=\u001b[33m'\u001b[39;49;00m\u001b[33mmax_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \n",
      "        max_length=max_seq_length, \n",
      "        truncation=\u001b[34mTrue\u001b[39;49;00m, \n",
      "        return_tensors=\u001b[33m\"\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    \u001b[37m# The id from the pre-trained BERT vocabulary that represents the token.  (Padding of 0 will be used if the # of tokens is less than `max_seq_length`)\u001b[39;49;00m\n",
      "    input_ids = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1).  Padded `input_ids` will have 0 in each of these vector elements.\u001b[39;49;00m\n",
      "    input_mask = encode_plus_tokens[\u001b[33m\"\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "    outputs = model.predict(x=(input_ids, input_mask))\n",
      "\n",
      "    prediction = [{\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: config.id2label[item.argmax()], \u001b[33m\"\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: item.max().item()} \u001b[34mfor\u001b[39;49;00m item \u001b[35min\u001b[39;49;00m outputs]\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m prediction[\u001b[34m0\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprocess\u001b[39;49;00m(args):\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mCurrent host: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.current_host))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_data: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_data))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_model: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_model))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of input model dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_model))\n",
      "    input_files = os.listdir(args.input_model)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m input_files:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "    model_tar_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/model.tar.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_model)\n",
      "    model_tar = tarfile.open(model_tar_path)\n",
      "    model_tar.extractall(args.input_model)\n",
      "    model_tar.close()\n",
      "\n",
      "    model = keras.models.load_model(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/tensorflow/saved_model/0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_model))\n",
      "    \u001b[36mprint\u001b[39;49;00m(model)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\n",
      "        \u001b[33m\"\"\"I loved it!  I will recommend this to everyone.\"\"\"\u001b[39;49;00m,\n",
      "        predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, model, args.max_seq_length),\n",
      "    )\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, model, args.max_seq_length))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\n",
      "        \u001b[33m\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"\u001b[39;49;00m,\n",
      "        predict(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, model, args.max_seq_length),\n",
      "    )\n",
      "\n",
      "    \u001b[37m###########################################################################################\u001b[39;49;00m\n",
      "    \u001b[37m# TODO:  Replace this with glob for all files and remove test_data/ from the model.tar.gz #\u001b[39;49;00m\n",
      "    \u001b[37m###########################################################################################\u001b[39;49;00m\n",
      "    \u001b[37m#    evaluation_data_path = '/opt/ml/processing/input/data/'\u001b[39;49;00m\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of input data dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_data))\n",
      "    input_files = os.listdir(args.input_data)\n",
      "\n",
      "    test_data_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.input_data)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUsing only \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m to evaluate.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(test_data_path))\n",
      "    df_test_reviews = pd.read_csv(test_data_path, delimiter=\u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, quoting=csv.QUOTE_NONE, compression=\u001b[33m\"\u001b[39;49;00m\u001b[33mgzip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)[\n",
      "        [\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    ]\n",
      "\n",
      "    df_test_reviews = df_test_reviews.sample(n=\u001b[34m100\u001b[39;49;00m)\n",
      "    df_test_reviews.shape\n",
      "    df_test_reviews.head()\n",
      "\n",
      "    y_test = [predict(review, model, args.max_seq_length) \u001b[34mfor\u001b[39;49;00m review \u001b[35min\u001b[39;49;00m df_test_reviews[\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]]\n",
      "    y_test\n",
      "\n",
      "    y_actual = df_test_reviews[\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    y_actual\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(classification_report(y_true=y_test, y_pred=y_actual))\n",
      "\n",
      "    accuracy = accuracy_score(y_true=y_test, y_pred=y_actual)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTest accuracy: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, accuracy)\n",
      "\n",
      "\u001b[37m#     def plot_conf_mat(cm, classes, title, cmap):\u001b[39;49;00m\n",
      "\u001b[37m#         print(cm)\u001b[39;49;00m\n",
      "\u001b[37m#         plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\u001b[39;49;00m\n",
      "\u001b[37m#         plt.title(title)\u001b[39;49;00m\n",
      "\u001b[37m#         plt.colorbar()\u001b[39;49;00m\n",
      "\u001b[37m#         tick_marks = np.arange(len(classes))\u001b[39;49;00m\n",
      "\u001b[37m#         plt.xticks(tick_marks, classes, rotation=45)\u001b[39;49;00m\n",
      "\u001b[37m#         plt.yticks(tick_marks, classes)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#         fmt = \"d\"\u001b[39;49;00m\n",
      "\u001b[37m#         thresh = cm.max() / 2.0\u001b[39;49;00m\n",
      "\u001b[37m#         for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\u001b[39;49;00m\n",
      "\u001b[37m#             plt.text(\u001b[39;49;00m\n",
      "\u001b[37m#                 j,\u001b[39;49;00m\n",
      "\u001b[37m#                 i,\u001b[39;49;00m\n",
      "\u001b[37m#                 format(cm[i, j], fmt),\u001b[39;49;00m\n",
      "\u001b[37m#                 horizontalalignment=\"center\",\u001b[39;49;00m\n",
      "\u001b[37m#                 color=\"black\" if cm[i, j] > thresh else \"black\",\u001b[39;49;00m\n",
      "\u001b[37m#             )\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#             plt.tight_layout()\u001b[39;49;00m\n",
      "\u001b[37m#             plt.ylabel(\"True label\")\u001b[39;49;00m\n",
      "\u001b[37m#             plt.xlabel(\"Predicted label\")\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     cm = confusion_matrix(y_true=y_test, y_pred=y_actual)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     plt.figure()\u001b[39;49;00m\n",
      "\u001b[37m#     fig, ax = plt.subplots(figsize=(10, 5))\u001b[39;49;00m\n",
      "\u001b[37m#     plot_conf_mat(cm, classes=CLASSES, title=\"Confusion Matrix\", cmap=plt.cm.Greens)\u001b[39;49;00m\n",
      "\n",
      "\u001b[37m#     # Save the confusion matrix\u001b[39;49;00m\n",
      "\u001b[37m#     plt.show()\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# Model Output\u001b[39;49;00m\n",
      "    metrics_path = os.path.join(args.output_data, \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics/\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    os.makedirs(metrics_path, exist_ok=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\u001b[37m#     plt.savefig(\"{}/confusion_matrix.png\".format(metrics_path))\u001b[39;49;00m\n",
      "\n",
      "    report_dict = {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmetrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "                \u001b[33m\"\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: accuracy,\n",
      "            },\n",
      "        },\n",
      "    }\n",
      "\n",
      "    evaluation_path = \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/evaluation.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(metrics_path)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(evaluation_path, \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        f.write(json.dumps(report_dict))\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of output dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.output_data))\n",
      "    output_files = os.listdir(args.output_data)\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m output_files:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mListing contents of output/metrics dir: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(metrics_path))\n",
      "    output_files = os.listdir(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(metrics_path))\n",
      "    \u001b[34mfor\u001b[39;49;00m file \u001b[35min\u001b[39;49;00m output_files:\n",
      "        \u001b[36mprint\u001b[39;49;00m(file)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mComplete\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    args = parse_args()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoaded arguments:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(args)\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment variables:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(os.environ)\n",
      "\n",
      "    process(args)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize evaluate_model_metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the processor instance to construct a `ProcessingStep`, along with the input and output channels and the code that will be executed when the pipeline invokes pipeline execution. This is very similar to a processor instance's `run` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "The `TrainingStep` and `ProcessingStep` `properties` attribute matches the object model of the [DescribeTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html) and  [DescribeProcessingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeProcessingJob.html) response objects, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(name=\"EvaluationReport\", output_name=\"metrics\", path=\"evaluation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_step = ProcessingStep(\n",
    "    name=\"EvaluateModel\",\n",
    "    processor=evaluation_processor,\n",
    "    code=\"evaluate_model_metrics.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/input/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=processing_step.properties.ProcessingInputs[\"raw-input-data\"].S3Input.S3Uri,\n",
    "            destination=\"/opt/ml/processing/input/data\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"metrics\", s3_upload_mode=\"EndOfJob\", source=\"/opt/ml/processing/output/metrics/\"\n",
    "        ),\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--max-seq-length\",\n",
    "        str(max_seq_length.default_value),\n",
    "    ],\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.model_metrics.ModelMetrics object at 0x7ff0b40fdc90>\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            evaluation_step.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register Model Step\n",
    "\n",
    "![](img/pipeline-5.png)\n",
    "\n",
    "We use the estimator instance that was used for the training step to construct an instance of `RegisterModel`. The result of executing `RegisterModel` in a pipeline is a Model Package. A Model Package is a reusable model artifacts abstraction that packages all ingredients necessary for inference. Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A Model Package Group is a collection of Model Packages. You can create a Model Package Group for a specific ML business problem, and you can keep adding versions/model packages into it. Typically, we expect customers to create a ModelPackageGroup for a SageMaker Workflow Pipeline so that they can keep adding versions/model packages to the group for every Workflow Pipeline run.\n",
    "\n",
    "The construction of `RegisterModel` is very similar to an estimator instance's `register` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "In particular, we pass in the `S3ModelArtifacts` from the `TrainingStep`, `step_train` properties. The `TrainingStep` `properties` attribute matches the object model of the [DescribeTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html) response object.\n",
    "\n",
    "Of note, we provided a specific model package group name which we will use in the Model Registry and CI/CD work later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")\n",
    "\n",
    "deploy_instance_type = ParameterString(name=\"DeployInstanceType\", default_value=\"ml.m4.xlarge\")\n",
    "\n",
    "deploy_instance_count = ParameterInteger(name=\"DeployInstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-Reviews-1710454716\n"
     ]
    }
   ],
   "source": [
    "model_package_group_name = f\"BERT-Reviews-{timestamp}\"\n",
    "\n",
    "print(model_package_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py37.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.1-cpu\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"tensorflow\",\n",
    "    region=region,\n",
    "    version=\"2.3.1\",\n",
    "    py_version=\"py37\",\n",
    "    instance_type=deploy_instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "print(inference_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    #    entry_point='inference.py', # Adds a Repack Step:  https://github.com/aws/sagemaker-python-sdk/blob/01c6ee3a9ec1831e935e86df58cf70bc92ed1bbe/src/sagemaker/workflow/_utils.py#L44\n",
    "    #    source_dir='src',\n",
    "    estimator=estimator,\n",
    "    image_uri=inference_image_uri,  # we have to specify, by default it's using training image\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/jsonlines\"],\n",
    "    response_types=[\"application/jsonlines\"],\n",
    "    inference_instances=[deploy_instance_type],\n",
    "    transform_instances=[\"ml.m4.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model for Deployment Step\n",
    "\n",
    "![](img/pipeline-5.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = \"bert-model-{}\".format(timestamp)\n",
    "\n",
    "model = Model(\n",
    "    name=model_name,\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=training_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "\n",
    "create_inputs = CreateModelInput(\n",
    "    instance_type=deploy_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "create_step = CreateModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    model=model,\n",
    "    inputs=create_inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Condition Step to Check Accuracy and Conditionally Register Model\n",
    "\n",
    "![](img/pipeline-6.png)\n",
    "\n",
    "Finally, we'd like to only register this model if the accuracy of the model, as determined by our evaluation step `step_eval`, exceeded some value. A `ConditionStep` allows for pipelines to support conditional execution in the pipeline DAG based on conditions of step properties. \n",
    "\n",
    "Below, we:\n",
    "\n",
    "* define a `ConditionGreaterThan` on the accuracy value found in the output of the evaluation step, `step_eval`.\n",
    "* use the condition in the list of conditions in a `ConditionStep`\n",
    "* pass the `RegisterModel` step collection into the `if_steps` of the `ConditionStep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_accuracy_value = ParameterFloat(name=\"MinAccuracyValue\", default_value=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import (\n",
    "    ConditionStep,\n",
    "    JsonGet,\n",
    ")\n",
    "\n",
    "minimum_accuracy_condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=evaluation_step,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"metrics.accuracy.value\",\n",
    "    ),\n",
    "    right=min_accuracy_value,  # accuracy\n",
    ")\n",
    "\n",
    "minimum_accuracy_condition_step = ConditionStep(\n",
    "    name=\"AccuracyCondition\",\n",
    "    conditions=[minimum_accuracy_condition],\n",
    "    if_steps=[register_step, create_step],  # success, continue with model registration\n",
    "    else_steps=[],  # fail, end the pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Pipeline of Parameters, Steps, and Conditions\n",
    "\n",
    "Let's tie it all up into a workflow pipeline so we can execute it, and even schedule it.\n",
    "\n",
    "A pipeline requires a `name`, `parameters`, and `steps`. Names must be unique within an `(account, region)` pair so we tack on the timestamp to the name.\n",
    "\n",
    "Note:\n",
    "\n",
    "* All the parameters used in the definitions must be present.\n",
    "* Steps passed into the pipeline need not be in the order of execution. The SageMaker Workflow service will resolve the _data dependency_ DAG as steps the execution complete.\n",
    "* Steps must be unique to either pipeline step list or a single condition step if/else list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        input_data,\n",
    "        processing_instance_count,\n",
    "        processing_instance_type,\n",
    "        max_seq_length,\n",
    "        balance_dataset,\n",
    "        train_split_percentage,\n",
    "        validation_split_percentage,\n",
    "        test_split_percentage,\n",
    "        feature_store_offline_prefix,\n",
    "        feature_group_name,\n",
    "        train_instance_type,\n",
    "        train_instance_count,\n",
    "        epochs,\n",
    "        learning_rate,\n",
    "        epsilon,\n",
    "        train_batch_size,\n",
    "        validation_batch_size,\n",
    "        test_batch_size,\n",
    "        train_steps_per_epoch,\n",
    "        validation_steps,\n",
    "        test_steps,\n",
    "        train_volume_size,\n",
    "        use_xla,\n",
    "        use_amp,\n",
    "        freeze_bert_layer,\n",
    "        enable_sagemaker_debugger,\n",
    "        enable_checkpointing,\n",
    "        enable_tensorboard,\n",
    "        input_mode,\n",
    "        run_validation,\n",
    "        run_test,\n",
    "        run_sample_predictions,\n",
    "        min_accuracy_value,\n",
    "        model_approval_status,\n",
    "        deploy_instance_type,\n",
    "        deploy_instance_count,\n",
    "    ],\n",
    "    steps=[processing_step, training_step, evaluation_step, minimum_accuracy_condition_step],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the Json of the pipeline definition that meets the SageMaker Workflow Pipeline DSL specification.\n",
    "\n",
    "By examining the definition, we're also confirming that the pipeline was well-defined, and that the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Metadata': {},\n",
      " 'Parameters': [{'DefaultValue': 's3://sagemaker-us-east-1-211125778552/amazon-reviews-pds/tsv/',\n",
      "                 'Name': 'InputData',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 1,\n",
      "                 'Name': 'ProcessingInstanceCount',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'ml.c5.2xlarge',\n",
      "                 'Name': 'ProcessingInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 64, 'Name': 'MaxSeqLength', 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'True',\n",
      "                 'Name': 'BalanceDataset',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 0.9,\n",
      "                 'Name': 'TrainSplitPercentage',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 0.05,\n",
      "                 'Name': 'ValidationSplitPercentage',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 0.05,\n",
      "                 'Name': 'TestSplitPercentage',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 'reviews-feature-store-1710454716',\n",
      "                 'Name': 'FeatureStoreOfflinePrefix',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'reviews-feature-group-1710454716',\n",
      "                 'Name': 'FeatureGroupName',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'ml.c5.9xlarge',\n",
      "                 'Name': 'TrainInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 1,\n",
      "                 'Name': 'TrainInstanceCount',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 1, 'Name': 'Epochs', 'Type': 'Integer'},\n",
      "                {'DefaultValue': 1e-05,\n",
      "                 'Name': 'LearningRate',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 1e-08, 'Name': 'Epsilon', 'Type': 'Float'},\n",
      "                {'DefaultValue': 128,\n",
      "                 'Name': 'TrainBatchSize',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 128,\n",
      "                 'Name': 'ValidationBatchSize',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 128,\n",
      "                 'Name': 'TestBatchSize',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 50,\n",
      "                 'Name': 'TrainStepsPerEpoch',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 50,\n",
      "                 'Name': 'ValidationSteps',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 50, 'Name': 'TestSteps', 'Type': 'Integer'},\n",
      "                {'DefaultValue': 1024,\n",
      "                 'Name': 'TrainVolumeSize',\n",
      "                 'Type': 'Integer'},\n",
      "                {'DefaultValue': 'True', 'Name': 'UseXLA', 'Type': 'String'},\n",
      "                {'DefaultValue': 'True', 'Name': 'UseAMP', 'Type': 'String'},\n",
      "                {'DefaultValue': 'False',\n",
      "                 'Name': 'FreezeBERTLayer',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'False',\n",
      "                 'Name': 'EnableSageMakerDebugger',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'False',\n",
      "                 'Name': 'EnableCheckpointing',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'False',\n",
      "                 'Name': 'EnableTensorboard',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'File', 'Name': 'InputMode', 'Type': 'String'},\n",
      "                {'DefaultValue': 'True',\n",
      "                 'Name': 'RunValidation',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'False', 'Name': 'RunTest', 'Type': 'String'},\n",
      "                {'DefaultValue': 'False',\n",
      "                 'Name': 'RunSamplePredictions',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 0.01,\n",
      "                 'Name': 'MinAccuracyValue',\n",
      "                 'Type': 'Float'},\n",
      "                {'DefaultValue': 'PendingManualApproval',\n",
      "                 'Name': 'ModelApprovalStatus',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 'ml.m4.xlarge',\n",
      "                 'Name': 'DeployInstanceType',\n",
      "                 'Type': 'String'},\n",
      "                {'DefaultValue': 1,\n",
      "                 'Name': 'DeployInstanceCount',\n",
      "                 'Type': 'Integer'}],\n",
      " 'Steps': [{'Arguments': {'AppSpecification': {'ContainerArguments': ['--train-split-percentage',\n",
      "                                                                      '0.9',\n",
      "                                                                      '--validation-split-percentage',\n",
      "                                                                      '0.05',\n",
      "                                                                      '--test-split-percentage',\n",
      "                                                                      '0.05',\n",
      "                                                                      '--max-seq-length',\n",
      "                                                                      '64',\n",
      "                                                                      '--balance-dataset',\n",
      "                                                                      'True',\n",
      "                                                                      '--feature-store-offline-prefix',\n",
      "                                                                      'reviews-feature-store-1710454716',\n",
      "                                                                      '--feature-group-name',\n",
      "                                                                      'reviews-feature-group-1710454716'],\n",
      "                                               'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert-feature-store.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
      "                          'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'raw-input-data',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/data/',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Get': 'Parameters.InputData'}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/input/code/preprocess-scikit-text-to-bert-feature-store.py'}}],\n",
      "                          'ProcessingOutputConfig': {'Outputs': [{'AppManaged': False,\n",
      "                                                                  'OutputName': 'bert-train',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/bert/train',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-train'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'bert-validation',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/bert/validation',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-validation'}},\n",
      "                                                                 {'AppManaged': False,\n",
      "                                                                  'OutputName': 'bert-test',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/bert/test',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-test'}}]},\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
      "                                                                    'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::211125778552:role/service-role/AmazonSageMaker-ExecutionRole-20240215T152311'},\n",
      "            'Name': 'Processing',\n",
      "            'Type': 'Processing'},\n",
      "           {'Arguments': {'AlgorithmSpecification': {'EnableSageMakerMetricsTimeSeries': True,\n",
      "                                                     'MetricDefinitions': [{'Name': 'train:loss',\n",
      "                                                                            'Regex': 'loss: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'train:accuracy',\n",
      "                                                                            'Regex': 'accuracy: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'validation:loss',\n",
      "                                                                            'Regex': 'val_loss: '\n",
      "                                                                                     '([0-9\\\\.]+)'},\n",
      "                                                                           {'Name': 'validation:accuracy',\n",
      "                                                                            'Regex': 'val_accuracy: '\n",
      "                                                                                     '([0-9\\\\.]+)'}],\n",
      "                                                     'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.3.1-cpu-py37',\n",
      "                                                     'TrainingInputMode': {'Get': 'Parameters.InputMode'}},\n",
      "                          'DebugHookConfig': {'CollectionConfigurations': [],\n",
      "                                              'S3OutputPath': 's3://sagemaker-us-east-1-211125778552'},\n",
      "                          'HyperParameters': {'enable_checkpointing': '\"False\"',\n",
      "                                              'enable_sagemaker_debugger': '\"False\"',\n",
      "                                              'enable_tensorboard': '\"False\"',\n",
      "                                              'epochs': '1',\n",
      "                                              'epsilon': '1e-08',\n",
      "                                              'freeze_bert_layer': '\"False\"',\n",
      "                                              'learning_rate': '1e-05',\n",
      "                                              'max_seq_length': '64',\n",
      "                                              'model_dir': '\"s3://sagemaker-us-east-1-211125778552/tensorflow-training-2024-03-14-22-18-41-414/model\"',\n",
      "                                              'run_sample_predictions': '\"False\"',\n",
      "                                              'run_test': '\"False\"',\n",
      "                                              'run_validation': '\"True\"',\n",
      "                                              'sagemaker_container_log_level': '20',\n",
      "                                              'sagemaker_job_name': '\"tensorflow-training-2024-03-14-22-18-41-414\"',\n",
      "                                              'sagemaker_program': '\"tf_bert_reviews.py\"',\n",
      "                                              'sagemaker_region': '\"us-east-1\"',\n",
      "                                              'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-211125778552/tensorflow-training-2024-03-14-22-18-41-414/source/sourcedir.tar.gz\"',\n",
      "                                              'test_batch_size': '128',\n",
      "                                              'test_steps': '50',\n",
      "                                              'train_batch_size': '128',\n",
      "                                              'train_steps_per_epoch': '50',\n",
      "                                              'use_amp': '\"True\"',\n",
      "                                              'use_xla': '\"True\"',\n",
      "                                              'validation_batch_size': '128',\n",
      "                                              'validation_steps': '50'},\n",
      "                          'InputDataConfig': [{'ChannelName': 'train',\n",
      "                                               'ContentType': 'text/csv',\n",
      "                                               'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                               'S3DataType': 'S3Prefix',\n",
      "                                                                               'S3Uri': {'Get': \"Steps.Processing.ProcessingOutputConfig.Outputs['bert-train'].S3Output.S3Uri\"}}}},\n",
      "                                              {'ChannelName': 'validation',\n",
      "                                               'ContentType': 'text/csv',\n",
      "                                               'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                               'S3DataType': 'S3Prefix',\n",
      "                                                                               'S3Uri': {'Get': \"Steps.Processing.ProcessingOutputConfig.Outputs['bert-validation'].S3Output.S3Uri\"}}}},\n",
      "                                              {'ChannelName': 'test',\n",
      "                                               'ContentType': 'text/csv',\n",
      "                                               'DataSource': {'S3DataSource': {'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                                               'S3DataType': 'S3Prefix',\n",
      "                                                                               'S3Uri': {'Get': \"Steps.Processing.ProcessingOutputConfig.Outputs['bert-test'].S3Output.S3Uri\"}}}}],\n",
      "                          'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-211125778552/'},\n",
      "                          'ProfilerConfig': {'ProfilingIntervalInMilliseconds': 500,\n",
      "                                             'ProfilingParameters': {'DataloaderProfilingConfig': '{\"StartStep\": '\n",
      "                                                                                                  '5, '\n",
      "                                                                                                  '\"NumSteps\": '\n",
      "                                                                                                  '10, '\n",
      "                                                                                                  '\"MetricsRegex\": '\n",
      "                                                                                                  '\".*\", '\n",
      "                                                                                                  '}',\n",
      "                                                                     'DetailedProfilingConfig': '{\"StartStep\": '\n",
      "                                                                                                '5, '\n",
      "                                                                                                '\"NumSteps\": '\n",
      "                                                                                                '10, '\n",
      "                                                                                                '}',\n",
      "                                                                     'FileOpenFailThreshold': '50',\n",
      "                                                                     'HorovodProfilingConfig': '{\"StartStep\": '\n",
      "                                                                                               '5, '\n",
      "                                                                                               '\"NumSteps\": '\n",
      "                                                                                               '10, '\n",
      "                                                                                               '}',\n",
      "                                                                     'LocalPath': '/opt/ml/output/profiler/',\n",
      "                                                                     'PythonProfilingConfig': '{\"StartStep\": '\n",
      "                                                                                              '5, '\n",
      "                                                                                              '\"NumSteps\": '\n",
      "                                                                                              '10, '\n",
      "                                                                                              '\"ProfilerName\": '\n",
      "                                                                                              '\"cprofile\", '\n",
      "                                                                                              '\"cProfileTimer\": '\n",
      "                                                                                              '\"total_time\", '\n",
      "                                                                                              '}',\n",
      "                                                                     'RotateFileCloseIntervalInSeconds': '60',\n",
      "                                                                     'RotateMaxFileSizeInBytes': '10485760',\n",
      "                                                                     'SMDataParallelProfilingConfig': '{\"StartStep\": '\n",
      "                                                                                                      '5, '\n",
      "                                                                                                      '\"NumSteps\": '\n",
      "                                                                                                      '10, '\n",
      "                                                                                                      '}'},\n",
      "                                             'S3OutputPath': 's3://sagemaker-us-east-1-211125778552/'},\n",
      "                          'ProfilerRuleConfigurations': [{'RuleConfigurationName': 'ProfilerReport',\n",
      "                                                          'RuleEvaluatorImage': '503895931360.dkr.ecr.us-east-1.amazonaws.com/sagemaker-debugger-rules:latest',\n",
      "                                                          'RuleParameters': {'rule_to_invoke': 'ProfilerReport'}}],\n",
      "                          'ResourceConfig': {'InstanceCount': {'Get': 'Parameters.TrainInstanceCount'},\n",
      "                                             'InstanceType': {'Get': 'Parameters.TrainInstanceType'},\n",
      "                                             'VolumeSizeInGB': {'Get': 'Parameters.TrainVolumeSize'}},\n",
      "                          'RoleArn': 'arn:aws:iam::211125778552:role/service-role/AmazonSageMaker-ExecutionRole-20240215T152311',\n",
      "                          'StoppingCondition': {'MaxRuntimeInSeconds': 86400}},\n",
      "            'CacheConfig': {'Enabled': True, 'ExpireAfter': 'PT1H'},\n",
      "            'Name': 'Train',\n",
      "            'Type': 'Training'},\n",
      "           {'Arguments': {'AppSpecification': {'ContainerArguments': ['--max-seq-length',\n",
      "                                                                      '64'],\n",
      "                                               'ContainerEntrypoint': ['python3',\n",
      "                                                                       '/opt/ml/processing/input/code/evaluate_model_metrics.py'],\n",
      "                                               'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'},\n",
      "                          'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'},\n",
      "                          'ProcessingInputs': [{'AppManaged': False,\n",
      "                                                'InputName': 'input-1',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/model',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Get': 'Steps.Train.ModelArtifacts.S3ModelArtifacts'}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'input-2',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/data',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': {'Get': \"Steps.Processing.ProcessingInputs['raw-input-data'].S3Input.S3Uri\"}}},\n",
      "                                               {'AppManaged': False,\n",
      "                                                'InputName': 'code',\n",
      "                                                'S3Input': {'LocalPath': '/opt/ml/processing/input/code',\n",
      "                                                            'S3CompressionType': 'None',\n",
      "                                                            'S3DataDistributionType': 'FullyReplicated',\n",
      "                                                            'S3DataType': 'S3Prefix',\n",
      "                                                            'S3InputMode': 'File',\n",
      "                                                            'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-43-057/input/code/evaluate_model_metrics.py'}}],\n",
      "                          'ProcessingOutputConfig': {'Outputs': [{'AppManaged': False,\n",
      "                                                                  'OutputName': 'metrics',\n",
      "                                                                  'S3Output': {'LocalPath': '/opt/ml/processing/output/metrics/',\n",
      "                                                                               'S3UploadMode': 'EndOfJob',\n",
      "                                                                               'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-40-486/output/metrics'}}]},\n",
      "                          'ProcessingResources': {'ClusterConfig': {'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
      "                                                                    'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
      "                                                                    'VolumeSizeInGB': 30}},\n",
      "                          'RoleArn': 'arn:aws:iam::211125778552:role/service-role/AmazonSageMaker-ExecutionRole-20240215T152311',\n",
      "                          'StoppingCondition': {'MaxRuntimeInSeconds': 7200}},\n",
      "            'Name': 'EvaluateModel',\n",
      "            'PropertyFiles': [{'FilePath': 'evaluation.json',\n",
      "                               'OutputName': 'metrics',\n",
      "                               'PropertyFileName': 'EvaluationReport'}],\n",
      "            'Type': 'Processing'},\n",
      "           {'Arguments': {'Conditions': [{'LeftValue': {'Std:JsonGet': {'Path': 'metrics.accuracy.value',\n",
      "                                                                        'PropertyFile': {'Get': 'Steps.EvaluateModel.PropertyFiles.EvaluationReport'}}},\n",
      "                                          'RightValue': {'Get': 'Parameters.MinAccuracyValue'},\n",
      "                                          'Type': 'GreaterThanOrEqualTo'}],\n",
      "                          'ElseSteps': [],\n",
      "                          'IfSteps': [{'Arguments': {'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.1-cpu',\n",
      "                                                                                                'ModelDataUrl': {'Get': 'Steps.Train.ModelArtifacts.S3ModelArtifacts'}}],\n",
      "                                                                                'SupportedContentTypes': ['application/jsonlines'],\n",
      "                                                                                'SupportedRealtimeInferenceInstanceTypes': [{'Get': 'Parameters.DeployInstanceType'}],\n",
      "                                                                                'SupportedResponseMIMETypes': ['application/jsonlines'],\n",
      "                                                                                'SupportedTransformInstanceTypes': ['ml.m4.xlarge']},\n",
      "                                                     'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'},\n",
      "                                                     'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
      "                                                                                                      'S3Uri': 's3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-40-486/output/metrics/evaluation.json'}}},\n",
      "                                                     'ModelPackageGroupName': 'BERT-Reviews-1710454716'},\n",
      "                                       'Name': 'RegisterModel',\n",
      "                                       'Type': 'RegisterModel'},\n",
      "                                      {'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::211125778552:role/service-role/AmazonSageMaker-ExecutionRole-20240215T152311',\n",
      "                                                     'PrimaryContainer': {'Environment': {},\n",
      "                                                                          'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.3.1-cpu',\n",
      "                                                                          'ModelDataUrl': {'Get': 'Steps.Train.ModelArtifacts.S3ModelArtifacts'}}},\n",
      "                                       'Name': 'CreateModel',\n",
      "                                       'Type': 'Model'}]},\n",
      "            'Name': 'AccuracyCondition',\n",
      "            'Type': 'Condition'}],\n",
      " 'Version': '2020-12-01'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "\n",
    "pprint(definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline to SageMaker and start execution\n",
    "\n",
    "Let's submit our pipeline definition to the workflow service. The role passed in will be used by the workflow service to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-pipeline-1710454714\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore the `WARNING` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:211125778552:pipeline/BERT-pipeline-1710454714\n"
     ]
    }
   ],
   "source": [
    "response = pipeline.create(role_arn=role)\n",
    "\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start the pipeline, accepting all the default parameters.\n",
    "\n",
    "Values can also be passed into these pipeline parameters on starting of the pipeline, and will be covered later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:211125778552:pipeline/BERT-pipeline-1710454714/execution/s7paou48eed0\n"
     ]
    }
   ],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        InputData=raw_input_data_s3_uri,\n",
    "        ProcessingInstanceCount=1,\n",
    "        ProcessingInstanceType=\"ml.c5.2xlarge\",\n",
    "        MaxSeqLength=64,\n",
    "        BalanceDataset=\"True\",\n",
    "        TrainSplitPercentage=0.9,\n",
    "        ValidationSplitPercentage=0.05,\n",
    "        TestSplitPercentage=0.05,\n",
    "        FeatureStoreOfflinePrefix=\"reviews-feature-store-\" + str(timestamp),\n",
    "        FeatureGroupName=\"reviews-feature-group-\" + str(timestamp),\n",
    "        LearningRate=0.000012,\n",
    "        TrainInstanceType=\"ml.c5.9xlarge\",\n",
    "        TrainInstanceCount=1,\n",
    "        Epochs=1,\n",
    "        Epsilon=0.00000001,\n",
    "        TrainBatchSize=128,\n",
    "        ValidationBatchSize=128,\n",
    "        TestBatchSize=128,\n",
    "        TrainStepsPerEpoch=50,\n",
    "        ValidationSteps=50,\n",
    "        TestSteps=50,\n",
    "        TrainVolumeSize=1024,\n",
    "        UseXLA=\"True\",\n",
    "        UseAMP=\"True\",\n",
    "        FreezeBERTLayer=\"False\",\n",
    "        EnableSageMakerDebugger=\"False\",\n",
    "        EnableCheckpointing=\"False\",\n",
    "        EnableTensorboard=\"False\",\n",
    "        InputMode=\"File\",\n",
    "        RunValidation=\"True\",\n",
    "        RunTest=\"False\",\n",
    "        RunSamplePredictions=\"False\",\n",
    "        MinAccuracyValue=0.01,\n",
    "        ModelApprovalStatus=\"PendingManualApproval\",\n",
    "        DeployInstanceType=\"ml.m4.xlarge\",\n",
    "        DeployInstanceCount=1,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(execution.arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow Operations: examining and waiting for pipeline execution\n",
    "\n",
    "Now we describe execution instance and list the steps in the execution to find out more about the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreatedBy': {'DomainId': 'd-k0wihxzgpgsi',\n",
      "               'IamIdentity': {'Arn': 'arn:aws:sts::211125778552:assumed-role/AmazonSageMaker-ExecutionRole-20240215T152311/SageMaker',\n",
      "                               'PrincipalId': 'AROATCKATXB4HYSGN6QQK:SageMaker'},\n",
      "               'UserProfileArn': 'arn:aws:sagemaker:us-east-1:211125778552:user-profile/d-k0wihxzgpgsi/default-user',\n",
      "               'UserProfileName': 'default-user'},\n",
      " 'CreationTime': datetime.datetime(2024, 3, 14, 22, 18, 45, 441000, tzinfo=tzlocal()),\n",
      " 'LastModifiedBy': {'DomainId': 'd-k0wihxzgpgsi',\n",
      "                    'IamIdentity': {'Arn': 'arn:aws:sts::211125778552:assumed-role/AmazonSageMaker-ExecutionRole-20240215T152311/SageMaker',\n",
      "                                    'PrincipalId': 'AROATCKATXB4HYSGN6QQK:SageMaker'},\n",
      "                    'UserProfileArn': 'arn:aws:sagemaker:us-east-1:211125778552:user-profile/d-k0wihxzgpgsi/default-user',\n",
      "                    'UserProfileName': 'default-user'},\n",
      " 'LastModifiedTime': datetime.datetime(2024, 3, 14, 22, 18, 45, 441000, tzinfo=tzlocal()),\n",
      " 'PipelineArn': 'arn:aws:sagemaker:us-east-1:211125778552:pipeline/BERT-pipeline-1710454714',\n",
      " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:211125778552:pipeline/BERT-pipeline-1710454714/execution/s7paou48eed0',\n",
      " 'PipelineExecutionDisplayName': 'execution-1710454725495',\n",
      " 'PipelineExecutionStatus': 'Executing',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '1069',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Thu, 14 Mar 2024 22:18:45 GMT',\n",
      "                                      'x-amzn-requestid': '0ff2d755-cb78-4a22-85f9-8f4fb993d546'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '0ff2d755-cb78-4a22-85f9-8f4fb993d546',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "execution_run = execution.describe()\n",
    "pprint(execution_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Execution Run as Trial to Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution-1710454725495\n"
     ]
    }
   ],
   "source": [
    "execution_run_name = execution_run[\"PipelineExecutionDisplayName\"]\n",
    "print(execution_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:211125778552:pipeline/BERT-pipeline-1710454714/execution/s7paou48eed0\n"
     ]
    }
   ],
   "source": [
    "pipeline_execution_arn = execution_run[\"PipelineExecutionArn\"]\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'StepName': 'Processing',\n",
       "  'StartTime': datetime.datetime(2024, 3, 14, 22, 18, 46, 76000, tzinfo=tzlocal()),\n",
       "  'StepStatus': 'Executing',\n",
       "  'AttemptCount': 1,\n",
       "  'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq'}}}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Giving the first step time to start up\n",
    "time.sleep(30)\n",
    "\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for the Pipeline to Complete\n",
    "\n",
    "# _Note: If this cell errors out with `WaiterError: Waiter PipelineExecutionComplete failed: Max attempts exceeded`, just re-run it and keep waiting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "Please wait...\n",
      "[{'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:211125778552:pipeline/BERT-pipeline-1710454714/execution/s7paou48eed0',\n",
      "  'PipelineExecutionDisplayName': 'execution-1710454725495',\n",
      "  'PipelineExecutionStatus': 'Succeeded',\n",
      "  'StartTime': datetime.datetime(2024, 3, 14, 22, 18, 45, 441000, tzinfo=tzlocal())}]\n",
      "CPU times: user 23.2 s, sys: 1.11 s, total: 24.3 s\n",
      "Wall time: 42min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)[\"PipelineExecutionSummaries\"]\n",
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)\n",
    "\n",
    "while pipeline_execution_status == \"Executing\":\n",
    "    try:\n",
    "        executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)[\"PipelineExecutionSummaries\"]\n",
    "        pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "    #        print('Executions for our pipeline...')\n",
    "    #        print(pipeline_execution_status)\n",
    "    except Exception as e:\n",
    "        print(\"Please wait...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "pprint(executions_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for the Pipeline ^^ Above ^^ to Complete\n",
    "\n",
    "# _Note: If this cell errors out with `WaiterError: Waiter PipelineExecutionComplete failed: Max attempts exceeded`, just re-run it and keep waiting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded\n"
     ]
    }
   ],
   "source": [
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sagemaker:us-east-1:211125778552:pipeline/BERT-pipeline-1710454714/execution/s7paou48eed0\n"
     ]
    }
   ],
   "source": [
    "pipeline_execution_arn = executions_response[0][\"PipelineExecutionArn\"]\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list the execution steps to check out the status and artifacts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Pipeline Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succeeded\n"
     ]
    }
   ],
   "source": [
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PipelineExecutionSteps': [{'AttemptCount': 1,\n",
      "                             'EndTime': datetime.datetime(2024, 3, 14, 23, 0, 55, 235000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:211125778552:model-package/BERT-Reviews-1710454716/1'}},\n",
      "                             'StartTime': datetime.datetime(2024, 3, 14, 23, 0, 54, 201000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'RegisterModel',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'AttemptCount': 1,\n",
      "                             'EndTime': datetime.datetime(2024, 3, 14, 23, 0, 55, 344000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:211125778552:model/pipelines-s7paou48eed0-createmodel-dtxkpwxj4c'}},\n",
      "                             'StartTime': datetime.datetime(2024, 3, 14, 23, 0, 54, 201000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'CreateModel',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'AttemptCount': 1,\n",
      "                             'EndTime': datetime.datetime(2024, 3, 14, 23, 0, 53, 729000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'Condition': {'Outcome': 'True'}},\n",
      "                             'StartTime': datetime.datetime(2024, 3, 14, 23, 0, 53, 409000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'AccuracyCondition',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'AttemptCount': 1,\n",
      "                             'EndTime': datetime.datetime(2024, 3, 14, 23, 0, 52, 365000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-EvaluateModel-U8RiAVLfZd'}},\n",
      "                             'StartTime': datetime.datetime(2024, 3, 14, 22, 48, 7, 713000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'EvaluateModel',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'AttemptCount': 1,\n",
      "                             'EndTime': datetime.datetime(2024, 3, 14, 22, 48, 6, 591000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:211125778552:training-job/pipelines-s7paou48eed0-Train-jZKKoTNirt'}},\n",
      "                             'StartTime': datetime.datetime(2024, 3, 14, 22, 32, 32, 350000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'Train',\n",
      "                             'StepStatus': 'Succeeded'},\n",
      "                            {'AttemptCount': 1,\n",
      "                             'EndTime': datetime.datetime(2024, 3, 14, 22, 32, 31, 313000, tzinfo=tzlocal()),\n",
      "                             'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq'}},\n",
      "                             'StartTime': datetime.datetime(2024, 3, 14, 22, 18, 46, 76000, tzinfo=tzlocal()),\n",
      "                             'StepName': 'Processing',\n",
      "                             'StepStatus': 'Succeeded'}],\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '1482',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Thu, 14 Mar 2024 23:01:20 GMT',\n",
      "                                      'x-amzn-requestid': '7f0638bc-7139-4afd-9dd6-3ace225b8c15'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': '7f0638bc-7139-4afd-9dd6-3ace225b8c15',\n",
      "                      'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "steps = sm.list_pipeline_execution_steps(PipelineExecutionArn=pipeline_execution_arn)\n",
    "\n",
    "pprint(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List All Artifacts Generated By The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_name = None\n",
    "training_job_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'Processing', 'StartTime': datetime.datetime(2024, 3, 14, 22, 18, 46, 76000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2024, 3, 14, 22, 32, 31, 313000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'AttemptCount': 1, 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq'}}}\n",
      "pipelines-s7paou48eed0-Processing-yEvdntPfGq\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...ess-scikit-text-to-bert-feature-store.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...t-1-211125778552/amazon-reviews-pds/tsv/</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>s3://...2024-03-14-22-18-41-139/output/bert-test</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...3-14-22-18-41-139/output/bert-validation</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s3://...024-03-14-22-18-41-139/output/bert-train</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...ess-scikit-text-to-bert-feature-store.py     Input  DataSet   \n",
       "1  s3://...t-1-211125778552/amazon-reviews-pds/tsv/     Input  DataSet   \n",
       "2  68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3     Input    Image   \n",
       "3  s3://...2024-03-14-22-18-41-139/output/bert-test    Output  DataSet   \n",
       "4  s3://...3-14-22-18-41-139/output/bert-validation    Output  DataSet   \n",
       "5  s3://...024-03-14-22-18-41-139/output/bert-train    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3         Produced     artifact  \n",
       "4         Produced     artifact  \n",
       "5         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'Train', 'StartTime': datetime.datetime(2024, 3, 14, 22, 32, 32, 350000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2024, 3, 14, 22, 48, 6, 591000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'AttemptCount': 1, 'Metadata': {'TrainingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:211125778552:training-job/pipelines-s7paou48eed0-Train-jZKKoTNirt'}}}\n",
      "pipelines-s7paou48eed0-Train-jZKKoTNirt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...2024-03-14-22-18-41-139/output/bert-test</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...3-14-22-18-41-139/output/bert-validation</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...024-03-14-22-18-41-139/output/bert-train</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76310...s.com/tensorflow-training:2.3.1-cpu-py37</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...ed0-Train-jZKKoTNirt/output/model.tar.gz</td>\n",
       "      <td>Output</td>\n",
       "      <td>Model</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...2024-03-14-22-18-41-139/output/bert-test     Input  DataSet   \n",
       "1  s3://...3-14-22-18-41-139/output/bert-validation     Input  DataSet   \n",
       "2  s3://...024-03-14-22-18-41-139/output/bert-train     Input  DataSet   \n",
       "3  76310...s.com/tensorflow-training:2.3.1-cpu-py37     Input    Image   \n",
       "4  s3://...ed0-Train-jZKKoTNirt/output/model.tar.gz    Output    Model   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'EvaluateModel', 'StartTime': datetime.datetime(2024, 3, 14, 22, 48, 7, 713000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2024, 3, 14, 23, 0, 52, 365000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'AttemptCount': 1, 'Metadata': {'ProcessingJob': {'Arn': 'arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-EvaluateModel-U8RiAVLfZd'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...620/input/code/evaluate_model_metrics.py</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s3://...t-1-211125778552/amazon-reviews-pds/tsv/</td>\n",
       "      <td>Input</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s3://...ed0-Train-jZKKoTNirt/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s3://...n-2024-03-14-22-18-40-486/output/metrics</td>\n",
       "      <td>Output</td>\n",
       "      <td>DataSet</td>\n",
       "      <td>Produced</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Name/Source Direction     Type  \\\n",
       "0  s3://...620/input/code/evaluate_model_metrics.py     Input  DataSet   \n",
       "1  s3://...t-1-211125778552/amazon-reviews-pds/tsv/     Input  DataSet   \n",
       "2  s3://...ed0-Train-jZKKoTNirt/output/model.tar.gz     Input    Model   \n",
       "3  68331...om/sagemaker-scikit-learn:0.23-1-cpu-py3     Input    Image   \n",
       "4  s3://...n-2024-03-14-22-18-40-486/output/metrics    Output  DataSet   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo     artifact  \n",
       "3    ContributedTo     artifact  \n",
       "4         Produced     artifact  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'AccuracyCondition', 'StartTime': datetime.datetime(2024, 3, 14, 23, 0, 53, 409000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2024, 3, 14, 23, 0, 53, 729000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'AttemptCount': 1, 'Metadata': {'Condition': {'Outcome': 'True'}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'CreateModel', 'StartTime': datetime.datetime(2024, 3, 14, 23, 0, 54, 201000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2024, 3, 14, 23, 0, 55, 344000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'AttemptCount': 1, 'Metadata': {'Model': {'Arn': 'arn:aws:sagemaker:us-east-1:211125778552:model/pipelines-s7paou48eed0-createmodel-dtxkpwxj4c'}}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'StepName': 'RegisterModel', 'StartTime': datetime.datetime(2024, 3, 14, 23, 0, 54, 201000, tzinfo=tzlocal()), 'EndTime': datetime.datetime(2024, 3, 14, 23, 0, 55, 235000, tzinfo=tzlocal()), 'StepStatus': 'Succeeded', 'AttemptCount': 1, 'Metadata': {'RegisterModel': {'Arn': 'arn:aws:sagemaker:us-east-1:211125778552:model-package/BERT-Reviews-1710454716/1'}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name/Source</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Type</th>\n",
       "      <th>Association Type</th>\n",
       "      <th>Lineage Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s3://...ed0-Train-jZKKoTNirt/output/model.tar.gz</td>\n",
       "      <td>Input</td>\n",
       "      <td>Model</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76310...onaws.com/tensorflow-inference:2.3.1-cpu</td>\n",
       "      <td>Input</td>\n",
       "      <td>Image</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>artifact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BERT-Reviews-1710454716-1-PendingManualApprova...</td>\n",
       "      <td>Input</td>\n",
       "      <td>Approval</td>\n",
       "      <td>ContributedTo</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BERT-Reviews-1710454716-1710457254-aws-model-p...</td>\n",
       "      <td>Output</td>\n",
       "      <td>ModelGroup</td>\n",
       "      <td>AssociatedWith</td>\n",
       "      <td>context</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name/Source Direction        Type  \\\n",
       "0   s3://...ed0-Train-jZKKoTNirt/output/model.tar.gz     Input       Model   \n",
       "1   76310...onaws.com/tensorflow-inference:2.3.1-cpu     Input       Image   \n",
       "2  BERT-Reviews-1710454716-1-PendingManualApprova...     Input    Approval   \n",
       "3  BERT-Reviews-1710454716-1710457254-aws-model-p...    Output  ModelGroup   \n",
       "\n",
       "  Association Type Lineage Type  \n",
       "0    ContributedTo     artifact  \n",
       "1    ContributedTo     artifact  \n",
       "2    ContributedTo       action  \n",
       "3   AssociatedWith      context  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(steps[\"PipelineExecutionSteps\"]):\n",
    "    print(execution_step)\n",
    "    # We are doing this because there appears to be a bug of this LineageTableVisualizer handling the Processing Step\n",
    "    if execution_step[\"StepName\"] == \"Processing\":\n",
    "        processing_job_name = execution_step[\"Metadata\"][\"ProcessingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(processing_job_name)\n",
    "        display(viz.show(processing_job_name=processing_job_name))\n",
    "    elif execution_step[\"StepName\"] == \"Train\":\n",
    "        training_job_name = execution_step[\"Metadata\"][\"TrainingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(training_job_name)\n",
    "        display(viz.show(training_job_name=training_job_name))\n",
    "    else:\n",
    "        display(viz.show(pipeline_execution_step=execution_step))\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Additional Parameters in our Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job\n"
     ]
    }
   ],
   "source": [
    "# -aws-processing-job is the default name assigned by ProcessingJob\n",
    "processing_job_tc = \"{}-aws-processing-job\".format(processing_job_name)\n",
    "print(processing_job_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_trial_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial-1710454714\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.associate_trial_component(TrialComponentName=processing_job_tc, TrialName=pipeline_trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipelines-s7paou48eed0-Train-jZKKoTNirt-aws-training-job\n"
     ]
    }
   ],
   "source": [
    "# -aws-training-job is the default name assigned by TrainingJob\n",
    "training_job_tc = \"{}-aws-training-job\".format(training_job_name)\n",
    "print(training_job_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.associate_trial_component(TrialComponentName=training_job_tc, TrialName=pipeline_trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments import tracker\n",
    "\n",
    "processing_job_tracker = tracker.Tracker.load(trial_component_name=processing_job_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialComponent(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7ff0b04991d0>,trial_component_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',trial_component_arn='arn:aws:sagemaker:us-east-1:211125778552:experiment-trial-component/pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',display_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',source=TrialComponentSource(source_arn='arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq',source_type='SageMakerProcessingJob'),status=TrialComponentStatus(primary_status='Completed',message='Status: Completed, exit message: null, failure reason: null'),start_time=datetime.datetime(2024, 3, 14, 22, 22, 52, tzinfo=tzlocal()),end_time=datetime.datetime(2024, 3, 14, 22, 32, 30, tzinfo=tzlocal()),creation_time=datetime.datetime(2024, 3, 14, 22, 18, 47, 224000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2024, 3, 14, 22, 32, 31, 81000, tzinfo=tzlocal()),last_modified_by={},parameters={'AWS_DEFAULT_REGION': 'us-east-1', 'SageMaker.InstanceCount': 1.0, 'SageMaker.InstanceType': 'ml.c5.2xlarge', 'SageMaker.VolumeSizeInGB': 30.0, 'balance_dataset': 'True'},input_artifacts={'SageMaker.ImageUri': TrialComponentArtifact(value='683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',media_type=None), 'code': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-43-288/input/code/preprocess-scikit-text-to-bert-feature-store.py',media_type=None), 'raw-input-data': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/amazon-reviews-pds/tsv/',media_type=None)},output_artifacts={'bert-test': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-test',media_type=None), 'bert-train': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-train',media_type=None), 'bert-validation': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-validation',media_type=None)},metrics=[],lineage_group_arn='arn:aws:sagemaker:us-east-1:211125778552:lineage-group/sagemaker-default-lineage-group',sources=[],response_metadata={'RequestId': '7b03a025-1ee1-455a-9758-b4a728d30dad', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '7b03a025-1ee1-455a-9758-b4a728d30dad', 'content-type': 'application/x-amz-json-1.1', 'content-length': '155', 'date': 'Thu, 14 Mar 2024 23:01:44 GMT'}, 'RetryAttempts': 0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"balance_dataset\": str(balance_dataset),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialComponent(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7ff0b04991d0>,trial_component_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',trial_component_arn='arn:aws:sagemaker:us-east-1:211125778552:experiment-trial-component/pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',display_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',source=TrialComponentSource(source_arn='arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq',source_type='SageMakerProcessingJob'),status=TrialComponentStatus(primary_status='Completed',message='Status: Completed, exit message: null, failure reason: null'),start_time=datetime.datetime(2024, 3, 14, 22, 22, 52, tzinfo=tzlocal()),end_time=datetime.datetime(2024, 3, 14, 22, 32, 30, tzinfo=tzlocal()),creation_time=datetime.datetime(2024, 3, 14, 22, 18, 47, 224000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2024, 3, 14, 22, 32, 31, 81000, tzinfo=tzlocal()),last_modified_by={},parameters={'AWS_DEFAULT_REGION': 'us-east-1', 'SageMaker.InstanceCount': 1.0, 'SageMaker.InstanceType': 'ml.c5.2xlarge', 'SageMaker.VolumeSizeInGB': 30.0, 'balance_dataset': 'True', 'train_split_percentage': '0.9'},input_artifacts={'SageMaker.ImageUri': TrialComponentArtifact(value='683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',media_type=None), 'code': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-43-288/input/code/preprocess-scikit-text-to-bert-feature-store.py',media_type=None), 'raw-input-data': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/amazon-reviews-pds/tsv/',media_type=None)},output_artifacts={'bert-test': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-test',media_type=None), 'bert-train': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-train',media_type=None), 'bert-validation': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-validation',media_type=None)},metrics=[],lineage_group_arn='arn:aws:sagemaker:us-east-1:211125778552:lineage-group/sagemaker-default-lineage-group',sources=[],response_metadata={'RequestId': 'e2196bbb-7e49-4f1b-8eea-043b0ff5a94e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'e2196bbb-7e49-4f1b-8eea-043b0ff5a94e', 'content-type': 'application/x-amz-json-1.1', 'content-length': '155', 'date': 'Thu, 14 Mar 2024 23:01:44 GMT'}, 'RetryAttempts': 0})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"train_split_percentage\": str(train_split_percentage),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialComponent(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7ff0b04991d0>,trial_component_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',trial_component_arn='arn:aws:sagemaker:us-east-1:211125778552:experiment-trial-component/pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',display_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',source=TrialComponentSource(source_arn='arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq',source_type='SageMakerProcessingJob'),status=TrialComponentStatus(primary_status='Completed',message='Status: Completed, exit message: null, failure reason: null'),start_time=datetime.datetime(2024, 3, 14, 22, 22, 52, tzinfo=tzlocal()),end_time=datetime.datetime(2024, 3, 14, 22, 32, 30, tzinfo=tzlocal()),creation_time=datetime.datetime(2024, 3, 14, 22, 18, 47, 224000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2024, 3, 14, 22, 32, 31, 81000, tzinfo=tzlocal()),last_modified_by={},parameters={'AWS_DEFAULT_REGION': 'us-east-1', 'SageMaker.InstanceCount': 1.0, 'SageMaker.InstanceType': 'ml.c5.2xlarge', 'SageMaker.VolumeSizeInGB': 30.0, 'balance_dataset': 'True', 'train_split_percentage': '0.9', 'validation_split_percentage': '0.05'},input_artifacts={'SageMaker.ImageUri': TrialComponentArtifact(value='683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',media_type=None), 'code': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-43-288/input/code/preprocess-scikit-text-to-bert-feature-store.py',media_type=None), 'raw-input-data': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/amazon-reviews-pds/tsv/',media_type=None)},output_artifacts={'bert-test': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-test',media_type=None), 'bert-train': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-train',media_type=None), 'bert-validation': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-validation',media_type=None)},metrics=[],lineage_group_arn='arn:aws:sagemaker:us-east-1:211125778552:lineage-group/sagemaker-default-lineage-group',sources=[],response_metadata={'RequestId': '51ff99fb-7e34-4c42-bdce-777b3a5b9e1c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '51ff99fb-7e34-4c42-bdce-777b3a5b9e1c', 'content-type': 'application/x-amz-json-1.1', 'content-length': '155', 'date': 'Thu, 14 Mar 2024 23:01:44 GMT'}, 'RetryAttempts': 0})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"validation_split_percentage\": str(validation_split_percentage),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialComponent(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7ff0b04991d0>,trial_component_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',trial_component_arn='arn:aws:sagemaker:us-east-1:211125778552:experiment-trial-component/pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',display_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',source=TrialComponentSource(source_arn='arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq',source_type='SageMakerProcessingJob'),status=TrialComponentStatus(primary_status='Completed',message='Status: Completed, exit message: null, failure reason: null'),start_time=datetime.datetime(2024, 3, 14, 22, 22, 52, tzinfo=tzlocal()),end_time=datetime.datetime(2024, 3, 14, 22, 32, 30, tzinfo=tzlocal()),creation_time=datetime.datetime(2024, 3, 14, 22, 18, 47, 224000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2024, 3, 14, 22, 32, 31, 81000, tzinfo=tzlocal()),last_modified_by={},parameters={'AWS_DEFAULT_REGION': 'us-east-1', 'SageMaker.InstanceCount': 1.0, 'SageMaker.InstanceType': 'ml.c5.2xlarge', 'SageMaker.VolumeSizeInGB': 30.0, 'balance_dataset': 'True', 'train_split_percentage': '0.9', 'validation_split_percentage': '0.05', 'test_split_percentage': '0.05'},input_artifacts={'SageMaker.ImageUri': TrialComponentArtifact(value='683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',media_type=None), 'code': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-43-288/input/code/preprocess-scikit-text-to-bert-feature-store.py',media_type=None), 'raw-input-data': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/amazon-reviews-pds/tsv/',media_type=None)},output_artifacts={'bert-test': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-test',media_type=None), 'bert-train': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-train',media_type=None), 'bert-validation': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-validation',media_type=None)},metrics=[],lineage_group_arn='arn:aws:sagemaker:us-east-1:211125778552:lineage-group/sagemaker-default-lineage-group',sources=[],response_metadata={'RequestId': '1087a09a-17ea-4128-96d6-f59b2f3a9751', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1087a09a-17ea-4128-96d6-f59b2f3a9751', 'content-type': 'application/x-amz-json-1.1', 'content-length': '155', 'date': 'Thu, 14 Mar 2024 23:01:44 GMT'}, 'RetryAttempts': 0})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"test_split_percentage\": str(test_split_percentage),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialComponent(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7ff0b04991d0>,trial_component_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',trial_component_arn='arn:aws:sagemaker:us-east-1:211125778552:experiment-trial-component/pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',display_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',source=TrialComponentSource(source_arn='arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq',source_type='SageMakerProcessingJob'),status=TrialComponentStatus(primary_status='Completed',message='Status: Completed, exit message: null, failure reason: null'),start_time=datetime.datetime(2024, 3, 14, 22, 22, 52, tzinfo=tzlocal()),end_time=datetime.datetime(2024, 3, 14, 22, 32, 30, tzinfo=tzlocal()),creation_time=datetime.datetime(2024, 3, 14, 22, 18, 47, 224000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2024, 3, 14, 22, 32, 31, 81000, tzinfo=tzlocal()),last_modified_by={},parameters={'AWS_DEFAULT_REGION': 'us-east-1', 'SageMaker.InstanceCount': 1.0, 'SageMaker.InstanceType': 'ml.c5.2xlarge', 'SageMaker.VolumeSizeInGB': 30.0, 'balance_dataset': 'True', 'train_split_percentage': '0.9', 'validation_split_percentage': '0.05', 'test_split_percentage': '0.05', 'max_seq_length': '64'},input_artifacts={'SageMaker.ImageUri': TrialComponentArtifact(value='683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',media_type=None), 'code': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-43-288/input/code/preprocess-scikit-text-to-bert-feature-store.py',media_type=None), 'raw-input-data': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/amazon-reviews-pds/tsv/',media_type=None)},output_artifacts={'bert-test': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-test',media_type=None), 'bert-train': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-train',media_type=None), 'bert-validation': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-validation',media_type=None)},metrics=[],lineage_group_arn='arn:aws:sagemaker:us-east-1:211125778552:lineage-group/sagemaker-default-lineage-group',sources=[],response_metadata={'RequestId': '635112d0-81e8-45b0-a5b0-f14eb93832b5', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '635112d0-81e8-45b0-a5b0-f14eb93832b5', 'content-type': 'application/x-amz-json-1.1', 'content-length': '155', 'date': 'Thu, 14 Mar 2024 23:01:44 GMT'}, 'RetryAttempts': 0})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"max_seq_length\": str(max_seq_length),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialComponent(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7ff0b04991d0>,trial_component_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',trial_component_arn='arn:aws:sagemaker:us-east-1:211125778552:experiment-trial-component/pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',display_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',source=TrialComponentSource(source_arn='arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq',source_type='SageMakerProcessingJob'),status=TrialComponentStatus(primary_status='Completed',message='Status: Completed, exit message: null, failure reason: null'),start_time=datetime.datetime(2024, 3, 14, 22, 22, 52, tzinfo=tzlocal()),end_time=datetime.datetime(2024, 3, 14, 22, 32, 30, tzinfo=tzlocal()),creation_time=datetime.datetime(2024, 3, 14, 22, 18, 47, 224000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2024, 3, 14, 22, 32, 31, 81000, tzinfo=tzlocal()),last_modified_by={},parameters={'AWS_DEFAULT_REGION': 'us-east-1', 'SageMaker.InstanceCount': 1.0, 'SageMaker.InstanceType': 'ml.c5.2xlarge', 'SageMaker.VolumeSizeInGB': 30.0, 'balance_dataset': 'True', 'train_split_percentage': '0.9', 'validation_split_percentage': '0.05', 'test_split_percentage': '0.05', 'max_seq_length': '64', 'feature_store_offline_prefix': 'reviews-feature-store-1710454716'},input_artifacts={'SageMaker.ImageUri': TrialComponentArtifact(value='683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',media_type=None), 'code': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-43-288/input/code/preprocess-scikit-text-to-bert-feature-store.py',media_type=None), 'raw-input-data': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/amazon-reviews-pds/tsv/',media_type=None)},output_artifacts={'bert-test': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-test',media_type=None), 'bert-train': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-train',media_type=None), 'bert-validation': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-validation',media_type=None)},metrics=[],lineage_group_arn='arn:aws:sagemaker:us-east-1:211125778552:lineage-group/sagemaker-default-lineage-group',sources=[],response_metadata={'RequestId': 'e3c5e085-0cbb-4939-ab32-5cca5d1697c0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'e3c5e085-0cbb-4939-ab32-5cca5d1697c0', 'content-type': 'application/x-amz-json-1.1', 'content-length': '155', 'date': 'Thu, 14 Mar 2024 23:01:49 GMT'}, 'RetryAttempts': 0})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(5)  # avoid throttling exception\n",
    "\n",
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"feature_store_offline_prefix\": str(feature_store_offline_prefix),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrialComponent(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7ff0b04991d0>,trial_component_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',trial_component_arn='arn:aws:sagemaker:us-east-1:211125778552:experiment-trial-component/pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',display_name='pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job',source=TrialComponentSource(source_arn='arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq',source_type='SageMakerProcessingJob'),status=TrialComponentStatus(primary_status='Completed',message='Status: Completed, exit message: null, failure reason: null'),start_time=datetime.datetime(2024, 3, 14, 22, 22, 52, tzinfo=tzlocal()),end_time=datetime.datetime(2024, 3, 14, 22, 32, 30, tzinfo=tzlocal()),creation_time=datetime.datetime(2024, 3, 14, 22, 18, 47, 224000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2024, 3, 14, 22, 32, 31, 81000, tzinfo=tzlocal()),last_modified_by={},parameters={'AWS_DEFAULT_REGION': 'us-east-1', 'SageMaker.InstanceCount': 1.0, 'SageMaker.InstanceType': 'ml.c5.2xlarge', 'SageMaker.VolumeSizeInGB': 30.0, 'balance_dataset': 'True', 'train_split_percentage': '0.9', 'validation_split_percentage': '0.05', 'test_split_percentage': '0.05', 'max_seq_length': '64', 'feature_store_offline_prefix': 'reviews-feature-store-1710454716', 'feature_group_name': 'reviews-feature-group-1710454716'},input_artifacts={'SageMaker.ImageUri': TrialComponentArtifact(value='683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3',media_type=None), 'code': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-43-288/input/code/preprocess-scikit-text-to-bert-feature-store.py',media_type=None), 'raw-input-data': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/amazon-reviews-pds/tsv/',media_type=None)},output_artifacts={'bert-test': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-test',media_type=None), 'bert-train': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-train',media_type=None), 'bert-validation': TrialComponentArtifact(value='s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-validation',media_type=None)},metrics=[],lineage_group_arn='arn:aws:sagemaker:us-east-1:211125778552:lineage-group/sagemaker-default-lineage-group',sources=[],response_metadata={'RequestId': '01116463-8b02-43a4-8d91-32c2343bbff7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '01116463-8b02-43a4-8d91-32c2343bbff7', 'content-type': 'application/x-amz-json-1.1', 'content-length': '155', 'date': 'Thu, 14 Mar 2024 23:01:54 GMT'}, 'RetryAttempts': 0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(5)  # avoid throttling exception\n",
    "\n",
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"feature_group_name\": str(feature_group_name),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>AWS_DEFAULT_REGION</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>balance_dataset</th>\n",
       "      <th>feature_group_name</th>\n",
       "      <th>feature_store_offline_prefix</th>\n",
       "      <th>...</th>\n",
       "      <th>test - MediaType</th>\n",
       "      <th>test - Value</th>\n",
       "      <th>train - MediaType</th>\n",
       "      <th>train - Value</th>\n",
       "      <th>validation - MediaType</th>\n",
       "      <th>validation - Value</th>\n",
       "      <th>SageMaker.DebugHookOutput - MediaType</th>\n",
       "      <th>SageMaker.DebugHookOutput - Value</th>\n",
       "      <th>SageMaker.ModelArtifact - MediaType</th>\n",
       "      <th>SageMaker.ModelArtifact - Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job</td>\n",
       "      <td>pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq</td>\n",
       "      <td>us-east-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.c5.2xlarge</td>\n",
       "      <td>30.0</td>\n",
       "      <td>True</td>\n",
       "      <td>reviews-feature-group-1710454716</td>\n",
       "      <td>reviews-feature-store-1710454716</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pipelines-s7paou48eed0-Train-jZKKoTNirt-aws-training-job</td>\n",
       "      <td>pipelines-s7paou48eed0-Train-jZKKoTNirt-aws-training-job</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:211125778552:training-job/pipelines-s7paou48eed0-Train-jZKKoTNirt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.c5.9xlarge</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>text/csv</td>\n",
       "      <td>s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-test</td>\n",
       "      <td>text/csv</td>\n",
       "      <td>s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-train</td>\n",
       "      <td>text/csv</td>\n",
       "      <td>s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-validation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-east-1-211125778552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/output/model.tar.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TrialComponentName  \\\n",
       "0  pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job   \n",
       "1         pipelines-s7paou48eed0-Train-jZKKoTNirt-aws-training-job   \n",
       "\n",
       "                                                       DisplayName  \\\n",
       "0  pipelines-s7paou48eed0-Processing-yEvdntPfGq-aws-processing-job   \n",
       "1         pipelines-s7paou48eed0-Train-jZKKoTNirt-aws-training-job   \n",
       "\n",
       "                                                                                              SourceArn  \\\n",
       "0  arn:aws:sagemaker:us-east-1:211125778552:processing-job/pipelines-s7paou48eed0-Processing-yEvdntPfGq   \n",
       "1         arn:aws:sagemaker:us-east-1:211125778552:training-job/pipelines-s7paou48eed0-Train-jZKKoTNirt   \n",
       "\n",
       "  AWS_DEFAULT_REGION  SageMaker.InstanceCount SageMaker.InstanceType  \\\n",
       "0          us-east-1                      1.0          ml.c5.2xlarge   \n",
       "1                NaN                      1.0          ml.c5.9xlarge   \n",
       "\n",
       "   SageMaker.VolumeSizeInGB balance_dataset                feature_group_name  \\\n",
       "0                      30.0            True  reviews-feature-group-1710454716   \n",
       "1                    1024.0             NaN                               NaN   \n",
       "\n",
       "       feature_store_offline_prefix  ... test - MediaType  \\\n",
       "0  reviews-feature-store-1710454716  ...              NaN   \n",
       "1                               NaN  ...         text/csv   \n",
       "\n",
       "                                                                                            test - Value  \\\n",
       "0                                                                                                    NaN   \n",
       "1  s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-test   \n",
       "\n",
       "  train - MediaType  \\\n",
       "0               NaN   \n",
       "1          text/csv   \n",
       "\n",
       "                                                                                            train - Value  \\\n",
       "0                                                                                                     NaN   \n",
       "1  s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-train   \n",
       "\n",
       "   validation - MediaType  \\\n",
       "0                     NaN   \n",
       "1                text/csv   \n",
       "\n",
       "                                                                                            validation - Value  \\\n",
       "0                                                                                                          NaN   \n",
       "1  s3://sagemaker-us-east-1-211125778552/sagemaker-scikit-learn-2024-03-14-22-18-41-139/output/bert-validation   \n",
       "\n",
       "   SageMaker.DebugHookOutput - MediaType  \\\n",
       "0                                    NaN   \n",
       "1                                    NaN   \n",
       "\n",
       "       SageMaker.DebugHookOutput - Value  SageMaker.ModelArtifact - MediaType  \\\n",
       "0                                    NaN                                  NaN   \n",
       "1  s3://sagemaker-us-east-1-211125778552                                  NaN   \n",
       "\n",
       "                                                                     SageMaker.ModelArtifact - Value  \n",
       "0                                                                                                NaN  \n",
       "1  s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/output/model.tar.gz  \n",
       "\n",
       "[2 rows x 135 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "time.sleep(30)  # avoid throttling exception\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 500)\n",
    "\n",
    "experiment_analytics = ExperimentAnalytics(\n",
    "    experiment_name=pipeline_experiment_name,\n",
    ")\n",
    "\n",
    "experiment_analytics_df = experiment_analytics.dataframe()\n",
    "experiment_analytics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze SageMaker Debugger Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-03-14 22:48:22 Starting - Preparing the instances for training\n",
      "2024-03-14 22:48:22 Downloading - Downloading the training image\n",
      "2024-03-14 22:48:22 Training - Training image download completed. Training in progress.\n",
      "2024-03-14 22:48:22 Uploading - Uploading generated training model\n",
      "2024-03-14 22:48:22 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "restored_estimator = sagemaker.estimator.Estimator.attach(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-211125778552?prefix=pipelines-s7paou48eed0-Train-jZKKoTNirt/\">S3 Debugger Output Data</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}?prefix={}/\">S3 Debugger Output Data</a></b>'.format(\n",
    "            bucket, restored_estimator.base_job_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download SageMaker Debugger Profiling Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_report_s3_uri = \"s3://{}/{}/rule-output/ProfilerReport/profiler-output\".format(\n",
    "    bucket, restored_estimator.base_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE profiler-reports/\n",
      "2024-03-14 22:48:12     344642 profiler-report.html\n",
      "2024-03-14 22:48:12     188860 profiler-report.ipynb\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $profiler_report_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-reports/Dataloader.json to profiler_report/profiler-reports/Dataloader.json\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-reports/BatchSize.json to profiler_report/profiler-reports/BatchSize.json\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-report.html to profiler_report/profiler-report.html\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-reports/GPUMemoryIncrease.json to profiler_report/profiler-reports/GPUMemoryIncrease.json\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-reports/LowGPUUtilization.json to profiler_report/profiler-reports/LowGPUUtilization.json\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallSystemUsage.json to profiler_report/profiler-reports/OverallSystemUsage.json\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-reports/CPUBottleneck.json to profiler_report/profiler-reports/CPUBottleneck.json\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-reports/IOBottleneck.json to profiler_report/profiler-reports/IOBottleneck.json\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-reports/LoadBalancing.json to profiler_report/profiler-reports/LoadBalancing.json\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallFrameworkMetrics.json to profiler_report/profiler-reports/OverallFrameworkMetrics.json\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-report.ipynb to profiler_report/profiler-report.ipynb\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-reports/StepOutlier.json to profiler_report/profiler-reports/StepOutlier.json\n",
      "download: s3://sagemaker-us-east-1-211125778552/pipelines-s7paou48eed0-Train-jZKKoTNirt/rule-output/ProfilerReport/profiler-output/profiler-reports/MaxInitializationTime.json to profiler_report/profiler-reports/MaxInitializationTime.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $profiler_report_s3_uri ./profiler_report/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"./profiler_report/profiler-report.html\">Profiler Report</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"./profiler_report/profiler-report.html\">Profiler Report</a></b>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Profiling Report in SM Studio\n",
    "![SageMaker Studio Extensions](img/studio_pipeline_training_debugger_assigned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Pipeline in SM Studio\n",
    "![SageMaker Studio Extensions](img/sm_studio_extensions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
       "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
       "\n",
       "<script>\n",
       "try {\n",
       "    els = document.getElementsByClassName(\"sm-command-button\");\n",
       "    els[0].click();\n",
       "}\n",
       "catch(err) {\n",
       "    // NoOp\n",
       "}    \n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "\n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\ntry {\n    Jupyter.notebook.save_checkpoint();\n    Jupyter.notebook.session.delete();\n}\ncatch(err) {\n    // NoOp\n}\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "try {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "    Jupyter.notebook.session.delete();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "data_science_on_aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
